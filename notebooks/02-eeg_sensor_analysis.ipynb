{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-03T01:17:38.849961Z",
     "start_time": "2018-01-03T01:17:36.717435Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/autofs/space/cassia_001/users/matt/software/anaconda2.7/envs/msit/lib/python2.7/site-packages/statsmodels/compat/pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../src\")\n",
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "import json\n",
    "import pandas as pd\n",
    "from grabbit import Layout\n",
    "from mne import read_epochs, grand_average, write_evokeds, read_evokeds\n",
    "from mne import pick_types, combine_evoked, set_log_level, grand_average\n",
    "from mne.time_frequency import tfr_morlet, read_tfrs, write_tfrs\n",
    "from mne.viz import plot_compare_evokeds\n",
    "from mne.channels import find_ch_connectivity\n",
    "from utils import CH_NAMES, select_subjects, drop_bad_trials\n",
    "from eeg_sensor_analysis import baseline_normalize, power_heatmap, add_events\n",
    "from surface_laplacian import surface_laplacian\n",
    "from mne.stats import spatio_temporal_cluster_1samp_test\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from ipywidgets import interact, fixed\n",
    "import statsmodels.api as sm\n",
    "\n",
    "sns.set(style='whitegrid', font_scale=2)\n",
    "colors = ['#e41a1c', '#377eb8', '#4daf4a', '#984ea3']\n",
    "set_log_level('critical')\n",
    "\n",
    "# load subjects to process\n",
    "layout = Layout('../data', '../data/grabbit_config.json')\n",
    "subjects = select_subjects(layout, 'eeg', exclude='eeg')\n",
    "\n",
    "# Analysis Parameters \n",
    "conditions = ['incongruent', 'congruent']\n",
    "epoch_types = ['stimulus', 'response']\n",
    "epoch_times = [(-.5, 1.75), (-1, 1)]\n",
    "baseline = (-.5, -.1)\n",
    "# match Cohen, Donner 2013 \n",
    "frequencies = np.logspace(np.log10(2), np.log10(60), num=30) \n",
    "n_cycles = np.logspace(np.log10(3), np.log10(10), num=30) \n",
    "\n",
    "# load behavior\n",
    "behavior = pd.read_csv('../data/derivatives/behavior/group_data.tsv', \n",
    "                       na_values='n/a', sep='\\t') \n",
    "behavior = behavior[behavior.modality == 'eeg']\n",
    "\n",
    "# make eeg_sensor derivative directory structure\n",
    "pipeline_root = '../data/derivatives/eeg_sensor'\n",
    "if not os.path.exists(pipeline_root):\n",
    "    os.makedirs(pipeline_root)\n",
    "if not os.path.exists('%s/stats' % pipeline_root):\n",
    "    os.makedirs('%s/stats' % pipeline_root)\n",
    "for subject in subjects + ['group']:\n",
    "    if not os.path.exists('%s/%s' % (pipeline_root, subject)):\n",
    "        os.makedirs('%s/%s' % (pipeline_root, subject))\n",
    "    if not os.path.exists('%s/%s/evoked' % (pipeline_root, subject)):\n",
    "        os.makedirs('%s/%s/evoked' % (pipeline_root, subject))\n",
    "    if not os.path.exists('%s/%s/tfr' % (pipeline_root, subject)):\n",
    "        os.makedirs('%s/%s/tfr' % (pipeline_root, subject))\n",
    "    if not os.path.exists('%s/%s/lap' % (pipeline_root, subject)):\n",
    "        os.makedirs('%s/%s/lap' % (pipeline_root, subject))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Surface Laplacian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-22T19:04:03.393818Z",
     "start_time": "2017-12-22T18:19:30.457895Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stimulus\n",
      "sub-hc001\n",
      "sub-hc002\n",
      "sub-hc003\n",
      "sub-hc004\n",
      "sub-hc005\n",
      "sub-hc006\n",
      "sub-hc007\n",
      "sub-hc008\n",
      "sub-hc009\n",
      "sub-hc010\n",
      "sub-hc011\n",
      "sub-hc012\n",
      "sub-hc014\n",
      "sub-hc015\n",
      "sub-hc016\n",
      "sub-hc017\n",
      "sub-hc019\n",
      "sub-hc020\n",
      "sub-hc021\n",
      "sub-hc022\n",
      "sub-hc023\n",
      "sub-hc024\n",
      "sub-hc025\n",
      "sub-hc026\n",
      "sub-hc028\n",
      "sub-hc029\n",
      "sub-hc030\n",
      "sub-hc031\n",
      "sub-hc032\n",
      "sub-hc033\n",
      "sub-hc034\n",
      "sub-hc035\n",
      "sub-hc036\n",
      "sub-hc037\n",
      "sub-hc042\n",
      "sub-hc044\n",
      "sub-hc045\n",
      "sub-pp001\n",
      "sub-pp002\n",
      "sub-pp003\n",
      "sub-pp004\n",
      "sub-pp005\n",
      "sub-pp006\n",
      "sub-pp007\n",
      "sub-pp008\n",
      "sub-pp009\n",
      "sub-pp010\n",
      "sub-pp011\n",
      "sub-pp012\n",
      "sub-pp013\n",
      "sub-pp014\n",
      "sub-pp015\n",
      "sub-pp016\n",
      "response\n",
      "sub-hc001\n",
      "sub-hc002\n",
      "sub-hc003\n",
      "sub-hc004\n",
      "sub-hc005\n",
      "sub-hc006\n",
      "sub-hc007\n",
      "sub-hc008\n",
      "sub-hc009\n",
      "sub-hc010\n",
      "sub-hc011\n",
      "sub-hc012\n",
      "sub-hc014\n",
      "sub-hc015\n",
      "sub-hc016\n",
      "sub-hc017\n",
      "sub-hc019\n",
      "sub-hc020\n",
      "sub-hc021\n",
      "sub-hc022\n",
      "sub-hc023\n",
      "sub-hc024\n",
      "sub-hc025\n",
      "sub-hc026\n",
      "sub-hc028\n",
      "sub-hc029\n",
      "sub-hc030\n",
      "sub-hc031\n",
      "sub-hc032\n",
      "sub-hc033\n",
      "sub-hc034\n",
      "sub-hc035\n",
      "sub-hc036\n",
      "sub-hc037\n",
      "sub-hc042\n",
      "sub-hc044\n",
      "sub-hc045\n",
      "sub-pp001\n",
      "sub-pp002\n",
      "sub-pp003\n",
      "sub-pp004\n",
      "sub-pp005\n",
      "sub-pp006\n",
      "sub-pp007\n",
      "sub-pp008\n",
      "sub-pp009\n",
      "sub-pp010\n",
      "sub-pp011\n",
      "sub-pp012\n",
      "sub-pp013\n",
      "sub-pp014\n",
      "sub-pp015\n",
      "sub-pp016\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "coord_file = '../data/eeg_spherical_coordinates.txt'\n",
    "coords = pd.read_csv(coord_file, names=['num', 'ch', 'x', 'y', 'z'], sep=',')\n",
    "x, y, z = np.array(coords.x), np.array(coords.y), np.array(coords.z)\n",
    "\n",
    "epo_root = '../data/derivatives/eeg_preprocessing'\n",
    "for epo_type, epo_times in zip(epoch_types, epoch_times):\n",
    "    print(epo_type)\n",
    "    \n",
    "    group = {'incongruent': [], 'congruent': []}\n",
    "    \n",
    "    for subject in subjects:\n",
    "        if subject == 'group':\n",
    "            continue\n",
    "        print(subject)\n",
    "            \n",
    "        # load subject epochs & behavior\n",
    "        f = '%s/%s/epochs/%s_%s_cleaned-epo.fif' % (epo_root, subject,\n",
    "                                                    subject, epo_type)\n",
    "        epochs = read_epochs(f, verbose=False)\n",
    "        sub_behavior = behavior[behavior.participant_id == subject]\n",
    "        \n",
    "        lap_epochs = surface_laplacian(epochs, x, y, z, 'epochs')\n",
    "        \n",
    "        f = '%s/%s/lap/%s_%s_lap-epo.fif' % (pipeline_root, subject, subject,\n",
    "                                             epo_type)\n",
    "        lap_epochs.save(f)\n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ERPs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Evoked Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-22T19:22:06.895782Z",
     "start_time": "2017-12-22T19:04:06.409440Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base\n",
      "stimulus\n",
      "sub-hc001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/autofs/space/cassia_001/users/matt/software/anaconda2.7/envs/msit/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sub-hc002\n",
      "sub-hc003\n",
      "sub-hc004\n",
      "sub-hc005\n",
      "sub-hc006\n",
      "sub-hc007\n",
      "sub-hc008\n",
      "sub-hc009\n",
      "sub-hc010\n",
      "sub-hc011\n",
      "sub-hc012\n",
      "sub-hc014\n",
      "sub-hc015\n",
      "sub-hc016\n",
      "sub-hc017\n",
      "sub-hc019\n",
      "sub-hc020\n",
      "sub-hc021\n",
      "sub-hc022\n",
      "sub-hc023\n",
      "sub-hc024\n",
      "sub-hc025\n",
      "sub-hc026\n",
      "sub-hc028\n",
      "sub-hc029\n",
      "sub-hc030\n",
      "sub-hc031\n",
      "sub-hc032\n",
      "sub-hc033\n",
      "sub-hc034\n",
      "sub-hc035\n",
      "sub-hc036\n",
      "sub-hc037\n",
      "sub-hc042\n",
      "sub-hc044\n",
      "sub-hc045\n",
      "sub-pp001\n",
      "sub-pp002\n",
      "sub-pp003\n",
      "sub-pp004\n",
      "sub-pp005\n",
      "sub-pp006\n",
      "sub-pp007\n",
      "sub-pp008\n",
      "sub-pp009\n",
      "sub-pp010\n",
      "sub-pp011\n",
      "sub-pp012\n",
      "sub-pp013\n",
      "sub-pp014\n",
      "sub-pp015\n",
      "sub-pp016\n",
      "response\n",
      "sub-hc001\n",
      "sub-hc002\n",
      "sub-hc003\n",
      "sub-hc004\n",
      "sub-hc005\n",
      "sub-hc006\n",
      "sub-hc007\n",
      "sub-hc008\n",
      "sub-hc009\n",
      "sub-hc010\n",
      "sub-hc011\n",
      "sub-hc012\n",
      "sub-hc014\n",
      "sub-hc015\n",
      "sub-hc016\n",
      "sub-hc017\n",
      "sub-hc019\n",
      "sub-hc020\n",
      "sub-hc021\n",
      "sub-hc022\n",
      "sub-hc023\n",
      "sub-hc024\n",
      "sub-hc025\n",
      "sub-hc026\n",
      "sub-hc028\n",
      "sub-hc029\n",
      "sub-hc030\n",
      "sub-hc031\n",
      "sub-hc032\n",
      "sub-hc033\n",
      "sub-hc034\n",
      "sub-hc035\n",
      "sub-hc036\n",
      "sub-hc037\n",
      "sub-hc042\n",
      "sub-hc044\n",
      "sub-hc045\n",
      "sub-pp001\n",
      "sub-pp002\n",
      "sub-pp003\n",
      "sub-pp004\n",
      "sub-pp005\n",
      "sub-pp006\n",
      "sub-pp007\n",
      "sub-pp008\n",
      "sub-pp009\n",
      "sub-pp010\n",
      "sub-pp011\n",
      "sub-pp012\n",
      "sub-pp013\n",
      "sub-pp014\n",
      "sub-pp015\n",
      "sub-pp016\n",
      "laplacian\n",
      "stimulus\n",
      "sub-hc001\n",
      "sub-hc002\n",
      "sub-hc003\n",
      "sub-hc004\n",
      "sub-hc005\n",
      "sub-hc006\n",
      "sub-hc007\n",
      "sub-hc008\n",
      "sub-hc009\n",
      "sub-hc010\n",
      "sub-hc011\n",
      "sub-hc012\n",
      "sub-hc014\n",
      "sub-hc015\n",
      "sub-hc016\n",
      "sub-hc017\n",
      "sub-hc019\n",
      "sub-hc020\n",
      "sub-hc021\n",
      "sub-hc022\n",
      "sub-hc023\n",
      "sub-hc024\n",
      "sub-hc025\n",
      "sub-hc026\n",
      "sub-hc028\n",
      "sub-hc029\n",
      "sub-hc030\n",
      "sub-hc031\n",
      "sub-hc032\n",
      "sub-hc033\n",
      "sub-hc034\n",
      "sub-hc035\n",
      "sub-hc036\n",
      "sub-hc037\n",
      "sub-hc042\n",
      "sub-hc044\n",
      "sub-hc045\n",
      "sub-pp001\n",
      "sub-pp002\n",
      "sub-pp003\n",
      "sub-pp004\n",
      "sub-pp005\n",
      "sub-pp006\n",
      "sub-pp007\n",
      "sub-pp008\n",
      "sub-pp009\n",
      "sub-pp010\n",
      "sub-pp011\n",
      "sub-pp012\n",
      "sub-pp013\n",
      "sub-pp014\n",
      "sub-pp015\n",
      "sub-pp016\n",
      "response\n",
      "sub-hc001\n",
      "sub-hc002\n",
      "sub-hc003\n",
      "sub-hc004\n",
      "sub-hc005\n",
      "sub-hc006\n",
      "sub-hc007\n",
      "sub-hc008\n",
      "sub-hc009\n",
      "sub-hc010\n",
      "sub-hc011\n",
      "sub-hc012\n",
      "sub-hc014\n",
      "sub-hc015\n",
      "sub-hc016\n",
      "sub-hc017\n",
      "sub-hc019\n",
      "sub-hc020\n",
      "sub-hc021\n",
      "sub-hc022\n",
      "sub-hc023\n",
      "sub-hc024\n",
      "sub-hc025\n",
      "sub-hc026\n",
      "sub-hc028\n",
      "sub-hc029\n",
      "sub-hc030\n",
      "sub-hc031\n",
      "sub-hc032\n",
      "sub-hc033\n",
      "sub-hc034\n",
      "sub-hc035\n",
      "sub-hc036\n",
      "sub-hc037\n",
      "sub-hc042\n",
      "sub-hc044\n",
      "sub-hc045\n",
      "sub-pp001\n",
      "sub-pp002\n",
      "sub-pp003\n",
      "sub-pp004\n",
      "sub-pp005\n",
      "sub-pp006\n",
      "sub-pp007\n",
      "sub-pp008\n",
      "sub-pp009\n",
      "sub-pp010\n",
      "sub-pp011\n",
      "sub-pp012\n",
      "sub-pp013\n",
      "sub-pp014\n",
      "sub-pp015\n",
      "sub-pp016\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "for epo_class in ['base', 'laplacian']:\n",
    "    print(epo_class)\n",
    "    \n",
    "    if epo_class == 'base':\n",
    "        epo_root = '../data/derivatives/eeg_preprocessing/%s/epochs/%s_%s_cleaned-epo.fif'\n",
    "    else:\n",
    "        epo_root = '../data/derivatives/eeg_sensor/%s/lap/%s_%s_lap-epo.fif'\n",
    "        \n",
    "    for epo_type, epo_times in zip(epoch_types, epoch_times):\n",
    "        print(epo_type)\n",
    "\n",
    "        group = {'incongruent': [], 'congruent': []}\n",
    "\n",
    "        for subject in subjects:\n",
    "            if subject == 'group':\n",
    "                continue\n",
    "\n",
    "            print(subject)\n",
    "\n",
    "            # load subject epochs & behavior\n",
    "            epo_file = epo_root % (subject, subject, epo_type)\n",
    "            epochs = read_epochs(epo_file, verbose=False)\n",
    "            sub_behavior = behavior[behavior.participant_id == subject]\n",
    "\n",
    "            # crop filter period\n",
    "            epochs.crop(epo_times[0], epo_times[1])\n",
    "\n",
    "            # drop bad trials from epochs and behavior\n",
    "            sub_behavior, epochs = drop_bad_trials(subject, sub_behavior,\n",
    "                                                   epochs, layout, epo_type)\n",
    "\n",
    "            # add event labels\n",
    "            epochs = add_events(epochs, sub_behavior)\n",
    "\n",
    "            # interpolate bads\n",
    "            bads = epochs.info['bads']\n",
    "            epochs.interpolate_bads(reset_bads=True)\n",
    "\n",
    "            # extract evoked and standard error\n",
    "            evos = [epochs[c].average() for c in conditions]\n",
    "            evos_std = [epochs[c].standard_error() for c in conditions]\n",
    "\n",
    "            # save evoked and standard error\n",
    "            f = '%s/%s/evoked/%s_%s_%s-ave.fif' % (pipeline_root, subject,\n",
    "                                                   subject, epo_type, epo_class)\n",
    "            write_evokeds(f, evos)\n",
    "            f = '%s/%s/evoked/%s_%s_%s_stderr-ave.fif' % (pipeline_root, subject,\n",
    "                                                           subject, epo_type, epo_class)\n",
    "            write_evokeds(f, evos_std)\n",
    "\n",
    "            # accumulate group data\n",
    "            for i, c in enumerate(conditions):\n",
    "                group[c].append(evos[i])\n",
    "\n",
    "        # accumulate group data\n",
    "        evos = []\n",
    "        evos_std = []\n",
    "        for i, c in enumerate(conditions):\n",
    "            evos.append(grand_average(group[c]))\n",
    "\n",
    "            # compute group standard error\n",
    "            tmp = np.array([e.data for e in group[c]])\n",
    "            tmp = np.std(tmp, axis=0) / np.sqrt(tmp.shape[0])\n",
    "\n",
    "            # place group standard error in evoked object\n",
    "            std_err = evos[i].copy()\n",
    "            std_err.data = tmp.squeeze()\n",
    "            evos_std.append(std_err)\n",
    "\n",
    "        # save group evoked and standard error\n",
    "        f = '%s/group/evoked/group_%s_%s-ave.fif' % (pipeline_root, epo_type,\n",
    "                                                     epo_class)\n",
    "        write_evokeds(f, evos)\n",
    "        f = '%s/group/evoked/group_%s_%s_stderr-ave.fif' % (pipeline_root, \n",
    "                                                            epo_type,\n",
    "                                                            epo_class)\n",
    "        write_evokeds(f, evos_std)\n",
    "\n",
    "del group\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Evoked Responses "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot ERP Waveforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-03T01:06:06.519529Z",
     "start_time": "2018-01-03T01:06:04.717854Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61e8953876a648aab695857dcb5dd6bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>interactive</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "interactive(children=(Dropdown(description=u'subject', options=('group', 'sub-hc001', 'sub-hc002', 'sub-hc003', 'sub-hc004', 'sub-hc005', 'sub-hc006', 'sub-hc007', 'sub-hc008', 'sub-hc009', 'sub-hc010', 'sub-hc011', 'sub-hc012', 'sub-hc014', 'sub-hc015', 'sub-hc016', 'sub-hc017', 'sub-hc019', 'sub-hc020', 'sub-hc021', 'sub-hc022', 'sub-hc023', 'sub-hc024', 'sub-hc025', 'sub-hc026', 'sub-hc028', 'sub-hc029', 'sub-hc030', 'sub-hc031', 'sub-hc032', 'sub-hc033', 'sub-hc034', 'sub-hc035', 'sub-hc036', 'sub-hc037', 'sub-hc042', 'sub-hc044', 'sub-hc045', 'sub-pp001', 'sub-pp002', 'sub-pp003', 'sub-pp004', 'sub-pp005', 'sub-pp006', 'sub-pp007', 'sub-pp008', 'sub-pp009', 'sub-pp010', 'sub-pp011', 'sub-pp012', 'sub-pp013', 'sub-pp014', 'sub-pp015', 'sub-pp016'), value='group'), Dropdown(description=u'ch', options=('Fp1', 'Fpz', 'Fp2', 'AF7', 'AF3', 'AFz', 'AF4', 'AF8', 'F7', 'F5', 'F3', 'F1', 'Fz', 'F2', 'F4', 'F6', 'F8', 'FT9', 'FT7', 'FC5', 'FC3', 'FC1', 'FCz', 'FC2', 'FC4', 'FC6', 'FT8', 'FT10', 'T9', 'T7', 'C5', 'C3', 'C1', 'Cz', 'C2', 'C4', 'C6', 'T8', 'T10', 'TP9', 'TP7', 'CP5', 'CP3', 'CP1', 'CPz', 'CP2', 'CP4', 'CP6', 'TP8', 'TP10', 'P9', 'P7', 'P5', 'P3', 'P1', 'Pz', 'P2', 'P4', 'P6', 'P8', 'P10', 'PO7', 'PO3', 'P0z', 'PO4', 'PO8', 'O1', 'Oz', 'O2', 'Iz'), value='Fp1'), Dropdown(description=u'epo_class', options=('base', 'laplacian'), value='base'), Dropdown(description=u'threshold', options=(0.001, 0.005, 0.01, 0.05), value=0.001), Output()), _dom_classes=('widget-interact',))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_erps(subject, ch, epo_class, behavior, threshold):\n",
    "    \n",
    "    plt.close('all')\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(24, 6), sharey=True)\n",
    "    \n",
    "    # remove bad trials from behavior\n",
    "    exclusions = ['fast_rt', 'no_response', 'error', 'post_error']\n",
    "    behavior = behavior.loc[np.where(np.sum(behavior[exclusions], \n",
    "                                            axis=1) == 0)[0], :]\n",
    "    if subject != 'group':\n",
    "        behavior = behavior.loc[behavior.participant_id == subject, :]\n",
    "    \n",
    "    \n",
    "    for i, epo_type in enumerate(epoch_types):\n",
    "        ax = axs[i]\n",
    "        \n",
    "        # load evoked with standard error \n",
    "        f = '%s/%s/evoked/%s_%s_%s-ave.fif' % (pipeline_root, subject,\n",
    "                                               subject, epo_type, epo_class)\n",
    "        evos = read_evokeds(f, verbose=False)\n",
    "        f = '%s/%s/evoked/%s_%s_%s_stderr-ave.fif' % (pipeline_root, subject,\n",
    "                                               subject, epo_type, epo_class)\n",
    "        evos_std = read_evokeds(f, verbose=False)\n",
    "        \n",
    "        if subject == 'group' and epo_type == 'stimulus':\n",
    "            # load clusters\n",
    "            f = '%s/stats/%s_stats.npz' % (pipeline_root, epo_type)\n",
    "            stats = np.load(f)\n",
    "\n",
    "            X = stats['X']\n",
    "            tfce = stats['tfce']\n",
    "            cluster_ps = stats['cluster_ps']\n",
    "        \n",
    "            ch_ix = evos[0].ch_names.index(ch)\n",
    "            times = evos[0].times\n",
    "            ps = cluster_ps[ch_ix, :]\n",
    "            for i, p in enumerate(ps):\n",
    "                if p < threshold:\n",
    "                    ax.axvline(times[i], color='k', alpha=0.02)\n",
    "        \n",
    "        for j, c in enumerate(conditions):\n",
    "            \n",
    "            evo = evos[j]\n",
    "            evo_std = evos_std[j]\n",
    "            \n",
    "            # select out chosen channel\n",
    "            evo.pick_channels([ch])\n",
    "            evo_std.pick_channels([ch])\n",
    "            \n",
    "            # extract the data and standard error\n",
    "            times = evo.times \n",
    "            data = evo.data.squeeze() * 1e6\n",
    "            std_err = evo_std.data.squeeze() * 1e6\n",
    "            \n",
    "            # plot waveforms with standard error shading\n",
    "            ax.plot(times, data, color=colors[j])\n",
    "            ax.fill_between(times, data - std_err, data + std_err,\n",
    "                            alpha=0.5, color=colors[j])\n",
    "            \n",
    "            \n",
    "        # histogram rts on bottom of stimulus-locked plots\n",
    "        for j, c in enumerate(conditions):\n",
    "            \n",
    "            if epo_type == 'stimulus':\n",
    "                bottom=ax.get_ylim()[0]\n",
    "                rt = behavior[behavior.trial_type == c].response_time\n",
    "                ax.hist(rt, color=colors[j], alpha=0.2, \n",
    "                        normed=True, bottom=bottom)\n",
    "            \n",
    "        # set time axis ticks\n",
    "        if epo_type == 'stimulus':\n",
    "            ax.set_xticks(np.arange(-.5, 1.8, .25))\n",
    "            ax.set_xlim((-.5, 1.75))\n",
    "            if epo_class == 'base':\n",
    "                ax.set_ylabel('$\\mu V$')\n",
    "            else:\n",
    "                ax.set_ylabel('$\\mu V / mm^2$')\n",
    "        else:\n",
    "            ax.set_xticks(np.arange(-1, 1.1, .25))\n",
    "        \n",
    "        # plot flourishes\n",
    "        ax.set_title('%s-locked' % epo_type)\n",
    "        ax.axvline(0, color='k')\n",
    "        ax.axhline(0, color='k')\n",
    "        ax.set_xlabel('Time (s)')\n",
    "        ax.legend(conditions, loc='best')\n",
    "    \n",
    "    plt.suptitle('%s %s %s ERPs' % (subject, ch, epo_class), y=1.05)\n",
    "    sns.despine()\n",
    "    plt.show();\n",
    "\n",
    "interact(plot_erps, subject=subjects, ch=CH_NAMES, \n",
    "         epo_class=['base', 'laplacian'], behavior=fixed(behavior),\n",
    "         threshold=[.001, .005, .01, .05]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Topomaps "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-28T19:00:33.735246Z",
     "start_time": "2017-12-28T19:00:28.479915Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5b1c8bcf6e344f99a3dcf974fc78dae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>interactive</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "interactive(children=(Dropdown(description=u'subject', options=('group', 'group', 'sub-hc001', 'sub-hc002', 'sub-hc003', 'sub-hc004', 'sub-hc005', 'sub-hc006', 'sub-hc007', 'sub-hc008', 'sub-hc009', 'sub-hc010', 'sub-hc011', 'sub-hc012', 'sub-hc014', 'sub-hc015', 'sub-hc016', 'sub-hc017', 'sub-hc019', 'sub-hc020', 'sub-hc021', 'sub-hc022', 'sub-hc023', 'sub-hc024', 'sub-hc025', 'sub-hc026', 'sub-hc028', 'sub-hc029', 'sub-hc030', 'sub-hc031', 'sub-hc032', 'sub-hc033', 'sub-hc034', 'sub-hc035', 'sub-hc036', 'sub-hc037', 'sub-hc042', 'sub-hc044', 'sub-hc045', 'sub-pp001', 'sub-pp002', 'sub-pp003', 'sub-pp004', 'sub-pp005', 'sub-pp006', 'sub-pp007', 'sub-pp008', 'sub-pp009', 'sub-pp010', 'sub-pp011', 'sub-pp012', 'sub-pp013', 'sub-pp014', 'sub-pp015', 'sub-pp016'), value='group'), Dropdown(description=u'epo_type', options=('response', 'stimulus'), value='response'), Dropdown(description=u'epo_class', options=('base', 'laplacian'), value='base'), Dropdown(description=u'condition', options=('incongruent', 'congruent', 'i-c'), value='incongruent'), Dropdown(description=u'col_limit', options=(0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0, 4.5), value=0.5), Output()), _dom_classes=('widget-interact',))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_topomap(subject, epo_type, epo_class, condition, col_limit):\n",
    "    plt.close('all')\n",
    "    \n",
    "    if epo_class == 'laplacian':\n",
    "        col_limit *= 10\n",
    "    \n",
    "    # load evoked \n",
    "    f = '%s/%s/evoked/%s_%s_%s-ave.fif' % (pipeline_root, subject,\n",
    "                                           subject, epo_type, epo_class)\n",
    "    evokeds = read_evokeds(f, verbose=False)\n",
    "    \n",
    "    if epo_type == 'stimulus':\n",
    "        times = np.arange(-.1, 1.5, .05)\n",
    "    else:\n",
    "        times = np.arange(-.8, .801, .05)\n",
    "    \n",
    "    if condition == 'incongruent':\n",
    "        evo = evokeds[0]\n",
    "    elif condition == 'congruent':\n",
    "        evo = evokeds[1]\n",
    "    else:\n",
    "        evo = combine_evoked(evokeds, weights=[1, -1])\n",
    "        \n",
    "    n_rows = len(times) // 5 + 1\n",
    "    f, axs = plt.subplots(n_rows, 5, figsize=(20, 14)) \n",
    "    \n",
    "    for i in range(n_rows):\n",
    "        ax_times = times[i * 5:(i + 1) * 5]\n",
    "        axes = axs[i, :len(ax_times)]\n",
    "        evo.plot_topomap(times=ax_times, axes=axes, colorbar=False, \n",
    "                         show=False, vmin=-col_limit, vmax=col_limit)\n",
    "        \n",
    "    plt.suptitle('%s %s %s %s ERP Topomaps (Col Lim: +- %.1f 20)' % (subject,\n",
    "                                                         epo_class,\n",
    "                                                         epo_type,\n",
    "                                                         condition,\n",
    "                                                         col_limit), y=1.05)\n",
    "    plt.show();\n",
    "\n",
    "interact(plot_topomap, subject=['group'] + subjects, \n",
    "         epo_type=['response', 'stimulus'],\n",
    "         epo_class=['base', 'laplacian'],\n",
    "         condition=['incongruent', 'congruent', 'i-c'],\n",
    "         col_limit=np.arange(.5, 5, .5),\n",
    "         diff_col_limit=np.arange(.5, 5, .5));\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFCE Permutation Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-01-03T01:17:47.638Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stimulus\n",
      "stat_fun(H1): min=-7.425959 max=7.981014\n",
      "Running initial clustering\n",
      "Using 80 thresholds from 0.00 to 7.90 for TFCE computation (h_power=2.00, e_power=0.50)\n",
      "Found 157570 clusters\n",
      "Permuting 4999 times...\n",
      "[                                        ] 0.19685 |    "
     ]
    }
   ],
   "source": [
    "threshold = {'step': .1, 'start': 0}\n",
    "\n",
    "# load ch_connectivity\n",
    "epo_type = 'stimulus'\n",
    "f = '%s/group/evoked/group_%s_laplacian-ave.fif' % (pipeline_root, epo_type)\n",
    "evokeds = read_evokeds(f, verbose=False)\n",
    "evokeds[0].interpolate_bads(reset_bads=True)\n",
    "connectivity, ch_names = find_ch_connectivity(evokeds[0].info, ch_type='eeg')\n",
    "\n",
    "for epo_type in epoch_types:\n",
    "    print(epo_type)\n",
    "    \n",
    "    # accumalate group data\n",
    "    group = []\n",
    "    for subject in subjects:\n",
    "        if subject == 'group':\n",
    "            continue\n",
    "\n",
    "        f = '%s/%s/evoked/%s_%s_laplacian-ave.fif' % (pipeline_root, subject,\n",
    "                                                      subject, epo_type)\n",
    "        evos = read_evokeds(f, verbose=False)\n",
    "        diff = evos[0].data - evos[1].data\n",
    "        group.append(diff.T)\n",
    "        \n",
    "    X = np.array(group)\n",
    "    \n",
    "    # spatio-temporal cluster testing\n",
    "    tfce, _, cluster_ps, perm_dist = spatio_temporal_cluster_1samp_test(X, verbose=True,\n",
    "                                                                        threshold=threshold,\n",
    "                                                                        n_permutations=5000, \n",
    "                                                                        connectivity=connectivity,\n",
    "                                                                        seed=5, \n",
    "                                                                        n_jobs=10) \n",
    "\n",
    "    cluster_ps = np.array(cluster_ps).reshape(tfce.shape).T\n",
    "    tfce = tfce.T\n",
    "    np.savez_compressed('%s/stats/%s_stats.npz' % (pipeline_root, epo_type), \n",
    "                        X=X, connectivity=connectivity, tfce=tfce,\n",
    "                        cluster_ps=cluster_ps,\n",
    "                        perm_dist=perm_dist)\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-02T20:47:21.360055Z",
     "start_time": "2018-01-02T20:47:21.349524Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(53, 2251, 70)\n",
      "(70, 70)\n",
      "(70, 2251)\n",
      "41986\n",
      "70\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(connectivity.shape)\n",
    "print(tfce.shape)\n",
    "print(np.sum(np.array(cluster_ps) <= .05))\n",
    "print(len(cluster_ps))\n",
    "print(len(perm_dist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-02T18:34:36.297261Z",
     "start_time": "2018-01-02T18:34:36.288548Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70, 2251)\n",
      "[[ 1.  1.  1. ...,  1.  1.  1.]\n",
      " [ 1.  1.  1. ...,  1.  1.  1.]\n",
      " [ 1.  1.  1. ...,  1.  1.  1.]\n",
      " ..., \n",
      " [ 1.  1.  1. ...,  1.  1.  1.]\n",
      " [ 1.  1.  1. ...,  1.  1.  1.]\n",
      " [ 1.  1.  1. ...,  1.  1.  1.]]\n"
     ]
    }
   ],
   "source": [
    "cluster_ps = np.array(cluster_ps).reshape(tfce.shape).T\n",
    "print(ps.shape)\n",
    "print(ps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Clusters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-02T20:47:27.809984Z",
     "start_time": "2018-01-02T20:47:27.262418Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import mne\n",
    "epo_type = 'stimulus'\n",
    "\n",
    "# load evoked \n",
    "f = '%s/group/evoked/group_%s_laplacian-ave.fif' % (pipeline_root, epo_type)\n",
    "evokeds = read_evokeds(f, verbose=False)\n",
    "evo = combine_evoked(evokeds, weights=[1, -1])\n",
    "\n",
    "# get sensor positions via layout\n",
    "pos = mne.find_layout(evokeds[0].info).pos\n",
    "\n",
    "# load clusters\n",
    "f = '%s/stats/%s_stats.npz' % (pipeline_root, epo_type)\n",
    "stats = np.load(f)\n",
    "\n",
    "# configure variables for visualization\n",
    "times = evokeds[0].times * 1e3\n",
    "\n",
    "X = stats['X']\n",
    "tfce = stats['tfce']\n",
    "cluster_ps = stats['cluster_ps']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-02T20:47:30.091845Z",
     "start_time": "2018-01-02T20:47:30.054745Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([ 0.001,  0.002,  0.003,  0.004,  0.005,  0.006,  0.007,  0.008,\n",
      "        0.009,  0.01 ,  0.011,  0.013,  0.014,  0.015,  0.016,  0.017,\n",
      "        0.018,  0.019,  0.02 ,  0.021,  0.022,  0.023,  0.024,  0.025,\n",
      "        0.026,  0.027,  0.029,  0.03 ,  0.031,  0.032,  0.033,  0.034,\n",
      "        0.035,  0.036,  0.037,  0.038,  0.039,  0.04 ,  0.041,  0.042,\n",
      "        0.044,  0.046,  0.047,  0.048,  0.049,  0.05 ,  0.051,  0.053,\n",
      "        0.054,  0.055,  0.056,  0.057,  0.058,  0.059,  0.061,  0.062,\n",
      "        0.063,  0.064,  0.065,  0.066,  0.068,  0.069,  0.07 ,  0.071,\n",
      "        0.072,  0.073,  0.074,  0.075,  0.076,  0.078,  0.079,  0.08 ,\n",
      "        0.084,  0.087,  0.088,  0.089,  0.091,  0.092,  0.093,  0.094,\n",
      "        0.095,  0.098,  0.1  ,  0.101,  0.102,  0.104,  0.105,  0.106,\n",
      "        0.107,  0.109,  0.112,  0.113,  0.114,  0.116,  0.118,  0.121,\n",
      "        0.122,  0.125,  0.126,  0.127,  0.128,  0.129,  0.13 ,  0.133,\n",
      "        0.134,  0.136,  0.138,  0.141,  0.142,  0.143,  0.146,  0.147,\n",
      "        0.148,  0.153,  0.154,  0.155,  0.157,  0.158,  0.159,  0.16 ,\n",
      "        0.161,  0.162,  0.163,  0.164,  0.165,  0.167,  0.168,  0.169,\n",
      "        0.171,  0.174,  0.175,  0.178,  0.179,  0.181,  0.184,  0.185,\n",
      "        0.186,  0.187,  0.188,  0.189,  0.19 ,  0.193,  0.195,  0.196,\n",
      "        0.199,  0.2  ,  0.201,  0.202,  0.203,  0.205,  0.206,  0.208,\n",
      "        0.209,  0.21 ,  0.211,  0.212,  0.216,  0.221,  0.222,  0.223,\n",
      "        0.226,  0.227,  0.23 ,  0.232,  0.234,  0.237,  0.239,  0.241,\n",
      "        0.242,  0.243,  0.244,  0.245,  0.246,  0.249,  0.251,  0.252,\n",
      "        0.257,  0.258,  0.26 ,  0.261,  0.262,  0.263,  0.265,  0.266,\n",
      "        0.267,  0.268,  0.269,  0.27 ,  0.272,  0.276,  0.277,  0.282,\n",
      "        0.285,  0.286,  0.288,  0.289,  0.29 ,  0.291,  0.293,  0.294,\n",
      "        0.296,  0.297,  0.298,  0.3  ,  0.301,  0.302,  0.303,  0.307,\n",
      "        0.309,  0.315,  0.316,  0.322,  0.332,  0.334,  0.338,  0.339,\n",
      "        0.341,  0.342,  0.343,  0.344,  0.346,  0.349,  0.355,  0.356,\n",
      "        0.358,  0.359,  0.36 ,  0.362,  0.364,  0.366,  0.367,  0.37 ,\n",
      "        0.371,  0.373,  0.375,  0.377,  0.379,  0.383,  0.384,  0.385,\n",
      "        0.387,  0.39 ,  0.392,  0.397,  0.398,  0.399,  0.4  ,  0.403,\n",
      "        0.407,  0.409,  0.413,  0.417,  0.423,  0.425,  0.432,  0.433,\n",
      "        0.436,  0.438,  0.441,  0.45 ,  0.452,  0.453,  0.454,  0.456,\n",
      "        0.457,  0.46 ,  0.462,  0.463,  0.47 ,  0.471,  0.472,  0.474,\n",
      "        0.476,  0.477,  0.479,  0.481,  0.482,  0.484,  0.487,  0.491,\n",
      "        0.492,  0.493,  0.494,  0.498,  0.499,  0.501,  0.503,  0.504,\n",
      "        0.505,  0.506,  0.509,  0.51 ,  0.511,  0.517,  0.518,  0.521,\n",
      "        0.523,  0.525,  0.527,  0.53 ,  0.532,  0.534,  0.535,  0.537,\n",
      "        0.538,  0.539,  0.543,  0.545,  0.546,  0.547,  0.548,  0.55 ,\n",
      "        0.551,  0.552,  0.557,  0.558,  0.561,  0.562,  0.563,  0.564,\n",
      "        0.566,  0.567,  0.57 ,  0.571,  0.572,  0.573,  0.576,  0.577,\n",
      "        0.579,  0.58 ,  0.581,  0.583,  0.584,  0.586,  0.589,  0.59 ,\n",
      "        0.592,  0.593,  0.594,  0.596,  0.597,  0.6  ,  0.603,  0.604,\n",
      "        0.606,  0.607,  0.609,  0.61 ,  0.614,  0.616,  0.62 ,  0.621,\n",
      "        0.625,  0.629,  0.63 ,  0.633,  0.635,  0.636,  0.637,  0.64 ,\n",
      "        0.642,  0.645,  0.655,  0.662,  0.663,  0.665,  0.668,  0.67 ,\n",
      "        0.671,  0.672,  0.675,  0.676,  0.677,  0.678,  0.679,  0.68 ,\n",
      "        0.685,  0.687,  0.689,  0.693,  0.695,  0.696,  0.699,  0.7  ,\n",
      "        0.701,  0.703,  0.704,  0.707,  0.714,  0.715,  0.716,  0.717,\n",
      "        0.718,  0.72 ,  0.721,  0.726,  0.727,  0.73 ,  0.731,  0.734,\n",
      "        0.736,  0.737,  0.738,  0.739,  0.74 ,  0.741,  0.742,  0.743,\n",
      "        0.744,  0.745,  0.749,  0.751,  0.752,  0.755,  0.756,  0.758,\n",
      "        0.759,  0.76 ,  0.763,  0.77 ,  0.771,  0.772,  0.773,  0.774,\n",
      "        0.775,  0.776,  0.777,  0.778,  0.779,  0.78 ,  0.781,  0.782,\n",
      "        0.783,  0.784,  0.785,  0.786,  0.788,  0.792,  0.795,  0.798,\n",
      "        0.8  ,  0.801,  0.802,  0.803,  0.804,  0.805,  0.806,  0.807,\n",
      "        0.808,  0.811,  0.812,  0.813,  0.814,  0.815,  0.816,  0.818,\n",
      "        0.819,  0.82 ,  0.821,  0.823,  0.824,  0.825,  0.826,  0.827,\n",
      "        0.828,  0.829,  0.83 ,  0.831,  0.833,  0.834,  0.837,  0.838,\n",
      "        0.839,  0.84 ,  0.841,  0.842,  0.843,  0.844,  0.845,  0.846,\n",
      "        0.847,  0.849,  0.85 ,  0.851,  0.852,  0.853,  0.854,  0.855,\n",
      "        0.856,  0.857,  0.859,  0.863,  0.864,  0.867,  0.868,  0.87 ,\n",
      "        0.871,  0.872,  0.873,  0.874,  0.875,  0.877,  0.878,  0.879,\n",
      "        0.88 ,  0.882,  0.883,  0.885,  0.887,  0.888,  0.889,  0.89 ,\n",
      "        0.892,  0.893,  0.895,  0.896,  0.9  ,  0.901,  0.902,  0.905,\n",
      "        0.906,  0.907,  0.908,  0.909,  0.911,  0.912,  0.913,  0.914,\n",
      "        0.916,  0.917,  0.918,  0.919,  0.921,  0.922,  0.923,  0.925,\n",
      "        0.926,  0.927,  0.928,  0.929,  0.931,  0.932,  0.934,  0.936,\n",
      "        0.937,  0.938,  0.939,  0.94 ,  0.942,  0.943,  0.944,  0.947,\n",
      "        0.948,  0.949,  0.951,  0.952,  0.953,  0.954,  0.955,  0.956,\n",
      "        0.958,  0.959,  0.96 ,  0.962,  0.963,  0.964,  0.965,  0.966,\n",
      "        0.967,  0.968,  0.969,  0.97 ,  0.971,  0.973,  0.974,  0.975,\n",
      "        0.976,  0.977,  0.978,  0.979,  0.98 ,  0.981,  0.982,  0.983,\n",
      "        0.984,  0.985,  0.986,  0.987,  0.988,  0.989,  0.99 ,  0.991,\n",
      "        0.992,  0.993,  0.994,  0.995,  0.996,  0.997,  0.998,  0.999,  1.   ]), array([25622,   506,  5133,    19,   696,     2,  1298,    30,    73,\n",
      "         142,    81,    35,  1827,    86,   260,    25,   275,   763,\n",
      "         163,    39,   114,  1496,    21,    19,     5,    15,    22,\n",
      "         237,    56,   688,    37,     2,    10,    48,   115,    27,\n",
      "          20,     1,    21,    12,    13,    65,     8,    90,  1610,\n",
      "         159,     8,    20,     4,     2,    65,    26,    20,     2,\n",
      "          52,   716,    13,   120,     3,    35,    84,    20,    27,\n",
      "           2,    24,   100,     7,    37,   146,    13,     2,    10,\n",
      "          17,    89,    10,    40,     8,    10,    32,    11,    10,\n",
      "          22,    54,     2,     1,    97,     4,     7,   148,     9,\n",
      "           8,    24,    47,    54,    44,    15,     3,    14,    33,\n",
      "        1417,    20,   136,    43,     6,   581,    23,    22,    11,\n",
      "           5,    26,    10,   198,     7,     3,     5,     5,    36,\n",
      "         131,     3,     1,    45,     1,     1,    15,     2,    36,\n",
      "          10,    25,     5,     5,     5,     2,     9,    10,     6,\n",
      "          51,     3,    23,   132,     6,    28,    14,    13,    33,\n",
      "         276,     5,    20,     4,    29,   176,     4,     6,    78,\n",
      "          17,     4,     3,    38,    13,     4,     8,     1,    12,\n",
      "          24,    19,    13,    22,     3,    42,    20,   310,     1,\n",
      "           7,     7,    43,    22,     9,    62,    11,     7,     4,\n",
      "         545,   159,     6,     3,    50,    54,    55,     5,    10,\n",
      "           1,     3,     4,     2,     5,     2,    30,   108,     5,\n",
      "           2,     6,    10,    18,     8,    43,     5,  1755,    69,\n",
      "           8,    13,     5,    50,   142,   157,    42,    84,     7,\n",
      "           7,     3,     1,     5,     1,     8,    53,     1,   159,\n",
      "           2,     5,    10,     7,     5,     3,     2,    96,    49,\n",
      "          16,     7,     2,     2,     6,     5,     2,     6,     6,\n",
      "           6,     6,    10,     5,     2,   337,     4,    17,     1,\n",
      "           4,   717,     1,     5,     3,     2,     2,     7,     3,\n",
      "          16,     1,     1,     7,     3,    10,     2,     5,     1,\n",
      "           3,     1,     3,     1,    20,     3,     1,     1,     1,\n",
      "           4,     3,     2,    32,    26,     3,    14,     2,    10,\n",
      "           9,     2,    24,     6,    17,     6,     3,   335,     3,\n",
      "           3,    30,    42,     3,     2,     5,    27,     3,    10,\n",
      "           9,     8,     1,     9,     4,     3,     5,    88,    20,\n",
      "          12,     8,    12,    15,  1856,     5,     3,     2,     3,\n",
      "          18,     7,    85,    14,    11,    23,     6,    78,     5,\n",
      "           3,     8,     4,    20,     7,     2,     2,     3,    48,\n",
      "          36,   149,     1,     1,     4,     3,     3,     2,     2,\n",
      "          42,    29,    14,     7,     4,    15,     4,     5,     2,\n",
      "        1312,    19,     2,    13,     2,     1,     2,    16,     2,\n",
      "           3,     3,     8,     1,     3,     1,    16,     3,     7,\n",
      "           2,     9,     4,     8,    11,     1,     4,    21,     5,\n",
      "           9,     6,     6,     4,    29,     2,    31,     8,    23,\n",
      "           6,     7,     5,    17,     8,     6,     2,     9,     6,\n",
      "           7,    11,     3,    23,    10,     6,    45,     5,     9,\n",
      "           7,    11,    22,     2,     4,    14,     3,     7,     8,\n",
      "           7,     5,    48,     7,     3,     6,    69,    13,    17,\n",
      "           2,    36,     2,     2,     7,    15,    49,    15,    49,\n",
      "          48,    10,     1,    10,    57,     6,    33,    38,   108,\n",
      "          24,     8,    11,     5,    36,    40,    24,     3,    11,\n",
      "          87,     7,     3,    46,    19,    51,     7,    11,    70,\n",
      "          64,    54,   120,   105,     7,    27,    79,     9,    15,\n",
      "          14,     1,     5,    28,    69,  1954,     7,     4,    30,\n",
      "          34,    45,   113,    77,    40,     7,    31,     8,    79,\n",
      "          73,     5,    23,    43,     3,    48,    11,    19,    59,\n",
      "          54,     7,    39,    82,   230,  1388,     3,     5,     9,\n",
      "          21,    11,    15,    89,     2,    66,     6,    32,    12,\n",
      "          31,     4,   219,    63,    17,    29,     3,     2,    68,\n",
      "          28,    25,    13,     9,     2,    61,     4,    15,     9,\n",
      "          70,    15,    93,    91,     8,    27,    21,    23,    18,\n",
      "         409,    11,    19,     1,   179,    40,    39,   184,    35,\n",
      "          41,     6,     6,    77,    10,     3,    24,    39,    94,\n",
      "           7,   610,    21,    13,    69,    13,    43,     3,     6,\n",
      "          53,    42,    26,    32,    92,    52,    26,   144,   146,\n",
      "         174,  2572,    65,    73,  1700,    36,   101,     3,   197,\n",
      "          37,   316,   375,   318,   126,   195, 81974]))\n"
     ]
    }
   ],
   "source": [
    "print(np.unique(cluster_ps, return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-02T20:49:53.039334Z",
     "start_time": "2018-01-02T20:49:52.821719Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZAAAAERCAYAAABVU/GxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQIiISBYGCBERycIAISIiWRggREQky/8DaZoHR3ckfpMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2acf670e50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(perm_dist)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-02T20:50:04.719385Z",
     "start_time": "2018-01-02T20:50:04.310627Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1dafbd7821f437780e81ca90e737578",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>interactive</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "interactive(children=(Dropdown(description=u'time', options=(-500.0, -499.0, -498.0, -497.0, -496.0, -495.0, -494.0, -493.0, -492.0, -491.0, -490.0, -489.0, -488.0, -487.0, -486.0, -485.0, -484.0, -483.0, -482.0, -481.0, -480.0, -479.0, -478.0, -477.0, -476.0, -475.0, -474.0, -473.0, -472.0, -471.0, -470.0, -469.0, -468.0, -467.0, -466.0, -465.0, -464.0, -463.0, -462.0, -461.0, -460.0, -459.0, -458.0, -457.0, -456.0, -455.0, -454.0, -453.0, -452.0, -451.0, -450.0, -449.0, -448.0, -447.0, -446.0, -445.0, -444.0, -443.0, -442.0, -441.0, -440.0, -439.0, -438.0, -437.0, -436.0, -435.0, -434.0, -433.0, -432.0, -431.0, -430.0, -429.0, -428.0, -427.0, -426.0, -425.0, -424.0, -423.0, -422.0, -421.0, -420.0, -419.0, -418.0, -417.0, -416.0, -415.0, -414.0, -413.0, -412.0, -411.0, -410.0, -409.0, -408.0, -407.0, -406.0, -405.0, -404.0, -403.0, -402.0, -401.0, -400.0, -399.0, -398.0, -397.0, -396.0, -395.0, -394.0, -393.0, -392.0, -391.0, -390.0, -389.0, -388.0, -387.0, -386.0, -385.0, -384.0, -383.0, -382.0, -381.0, -380.0, -379.0, -378.0, -377.0, -376.0, -375.0, -374.0, -373.0, -372.0, -371.0, -370.0, -369.0, -368.0, -367.0, -366.0, -365.0, -364.0, -363.0, -362.0, -361.0, -360.0, -359.0, -358.0, -357.0, -356.0, -355.0, -354.0, -353.0, -352.0, -351.0, -350.0, -349.0, -348.0, -347.0, -346.0, -345.0, -344.0, -343.0, -342.0, -341.0, -340.0, -339.0, -338.0, -337.0, -336.0, -335.0, -334.0, -333.0, -332.0, -331.0, -330.0, -329.0, -328.0, -327.0, -326.0, -325.0, -324.0, -323.0, -322.0, -321.0, -320.0, -319.0, -318.0, -317.0, -316.0, -315.0, -314.0, -313.0, -312.0, -311.0, -310.0, -309.0, -308.0, -307.0, -306.0, -305.0, -304.0, -303.0, -302.0, -301.0, -300.0, -299.0, -298.0, -297.0, -296.0, -295.0, -294.0, -293.0, -292.0, -291.0, -290.0, -289.0, -288.0, -287.0, -286.0, -285.0, -284.0, -283.0, -282.0, -281.0, -280.0, -279.0, -278.0, -277.0, -276.0, -275.0, -274.0, -273.0, -272.0, -271.0, -270.0, -269.0, -268.0, -267.0, -266.0, -265.0, -264.0, -263.0, -262.0, -261.0, -260.0, -259.0, -258.0, -257.0, -256.0, -255.0, -254.0, -253.0, -252.0, -251.0, -250.0, -249.0, -248.0, -247.0, -246.0, -245.0, -244.0, -243.0, -242.0, -241.0, -240.0, -239.0, -238.0, -237.0, -236.0, -235.0, -234.0, -233.0, -232.0, -231.0, -230.0, -229.0, -228.0, -227.0, -226.0, -225.0, -224.0, -223.0, -222.0, -221.0, -220.0, -219.0, -218.0, -217.0, -216.0, -215.0, -214.0, -213.0, -212.0, -211.0, -210.0, -209.0, -208.0, -207.0, -206.0, -205.0, -204.0, -203.0, -202.0, -201.0, -200.0, -199.0, -198.0, -197.0, -196.0, -195.0, -194.0, -193.0, -192.0, -191.0, -190.0, -189.0, -188.0, -187.0, -186.0, -185.0, -184.0, -183.0, -182.0, -181.0, -180.0, -179.0, -178.0, -177.0, -176.0, -175.0, -174.0, -173.0, -172.0, -171.0, -170.0, -169.0, -168.0, -167.0, -166.0, -165.0, -164.0, -163.0, -162.0, -161.0, -160.0, -159.0, -158.0, -157.0, -156.0, -155.0, -154.0, -153.0, -152.0, -151.0, -150.0, -149.0, -148.0, -147.0, -146.0, -145.0, -144.0, -143.0, -142.0, -141.0, -140.0, -139.0, -138.0, -137.0, -136.0, -135.0, -134.0, -133.0, -132.0, -131.0, -130.0, -129.0, -128.0, -127.0, -126.0, -125.0, -124.0, -123.0, -122.0, -121.0, -120.0, -119.0, -118.0, -117.0, -116.0, -115.0, -114.0, -113.0, -112.0, -111.0, -110.0, -109.0, -108.0, -107.0, -106.0, -105.0, -104.0, -103.0, -102.0, -101.0, -100.0, -99.0, -98.0, -97.0, -96.0, -95.0, -94.0, -93.0, -92.0, -91.0, -90.0, -89.0, -88.0, -87.0, -86.0, -85.0, -84.0, -83.0, -82.0, -81.0, -80.0, -79.0, -78.0, -77.0, -76.0, -75.0, -74.0, -73.0, -72.0, -71.0, -70.0, -69.0, -68.0, -67.0, -66.0, -65.0, -64.0, -63.0, -62.0, -61.0, -60.0, -59.0, -58.0, -57.0, -56.0, -55.0, -54.0, -53.0, -52.0, -51.0, -50.0, -49.0, -48.0, -47.0, -46.0, -45.0, -44.0, -43.0, -42.0, -41.0, -40.0, -39.0, -38.0, -37.0, -36.0, -35.0, -34.0, -33.0, -32.0, -31.0, -30.0, -29.0, -28.0, -27.0, -26.0, -25.0, -24.0, -23.0, -22.0, -21.0, -20.0, -19.0, -18.0, -17.0, -16.0, -15.0, -14.0, -13.0, -12.0, -11.0, -10.0, -9.0, -8.0, -7.0, -6.0, -5.0, -4.0, -3.0, -2.0, -1.0, 0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0, 170.0, 171.0, 172.0, 173.0, 174.0, 175.0, 176.0, 177.0, 178.0, 179.0, 180.0, 181.0, 182.0, 183.0, 184.0, 185.0, 186.0, 187.0, 188.0, 189.0, 190.0, 191.0, 192.0, 193.0, 194.0, 195.0, 196.0, 197.0, 198.0, 199.0, 200.0, 201.0, 202.0, 203.0, 204.0, 205.0, 206.0, 207.0, 208.0, 209.0, 210.0, 211.0, 212.0, 213.0, 214.0, 215.0, 216.0, 217.0, 218.0, 219.0, 220.0, 221.0, 222.0, 223.0, 224.0, 225.0, 226.0, 227.0, 228.0, 229.0, 230.0, 231.0, 232.0, 233.0, 234.0, 235.0, 236.0, 237.0, 238.0, 239.0, 240.0, 241.0, 242.0, 243.0, 244.0, 245.0, 246.0, 247.0, 248.0, 249.0, 250.0, 251.0, 252.0, 253.0, 254.0, 255.0, 256.0, 257.0, 258.0, 259.0, 260.0, 261.0, 262.0, 263.0, 264.0, 265.0, 266.0, 267.0, 268.0, 269.0, 270.0, 271.0, 272.0, 273.0, 274.0, 275.0, 276.0, 277.0, 278.0, 279.0, 280.0, 281.0, 282.0, 283.0, 284.0, 285.0, 286.0, 287.0, 288.0, 289.0, 290.0, 291.0, 292.0, 293.0, 294.0, 295.0, 296.0, 297.0, 298.0, 299.0, 300.0, 301.0, 302.0, 303.0, 304.0, 305.0, 306.0, 307.0, 308.0, 309.0, 310.0, 311.0, 312.0, 313.0, 314.0, 315.0, 316.0, 317.0, 318.0, 319.0, 320.0, 321.0, 322.0, 323.0, 324.0, 325.0, 326.0, 327.0, 328.0, 329.0, 330.0, 331.0, 332.0, 333.0, 334.0, 335.0, 336.0, 337.0, 338.0, 339.0, 340.0, 341.0, 342.0, 343.0, 344.0, 345.0, 346.0, 347.0, 348.0, 349.0, 350.0, 351.0, 352.0, 353.0, 354.0, 355.0, 356.0, 357.0, 358.0, 359.0, 360.0, 361.0, 362.0, 363.0, 364.0, 365.0, 366.0, 367.0, 368.0, 369.0, 370.0, 371.0, 372.0, 373.0, 374.0, 375.0, 376.0, 377.0, 378.0, 379.0, 380.0, 381.0, 382.0, 383.0, 384.0, 385.0, 386.0, 387.0, 388.0, 389.0, 390.0, 391.0, 392.0, 393.0, 394.0, 395.0, 396.0, 397.0, 398.0, 399.0, 400.0, 401.0, 402.0, 403.0, 404.0, 405.0, 406.0, 407.0, 408.0, 409.0, 410.0, 411.0, 412.0, 413.0, 414.0, 415.0, 416.0, 417.0, 418.0, 419.0, 420.0, 421.0, 422.0, 423.0, 424.0, 425.0, 426.0, 427.0, 428.0, 429.0, 430.0, 431.0, 432.0, 433.0, 434.0, 435.0, 436.0, 437.0, 438.0, 439.0, 440.0, 441.0, 442.0, 443.0, 444.0, 445.0, 446.0, 447.0, 448.0, 449.0, 450.0, 451.0, 452.0, 453.0, 454.0, 455.0, 456.0, 457.0, 458.0, 459.0, 460.0, 461.0, 462.0, 463.0, 464.0, 465.0, 466.0, 467.0, 468.0, 469.0, 470.0, 471.0, 472.0, 473.0, 474.0, 475.0, 476.0, 477.0, 478.0, 479.0, 480.0, 481.0, 482.0, 483.0, 484.0, 485.0, 486.0, 487.0, 488.0, 489.0, 490.0, 491.0, 492.0, 493.0, 494.0, 495.0, 496.0, 497.0, 498.0, 499.0, 500.0, 501.0, 502.0, 503.0, 504.0, 505.0, 506.0, 507.0, 508.0, 509.0, 510.0, 511.0, 512.0, 513.0, 514.0, 515.0, 516.0, 517.0, 518.0, 519.0, 520.0, 521.0, 522.0, 523.0, 524.0, 525.0, 526.0, 527.0, 528.0, 529.0, 530.0, 531.0, 532.0, 533.0, 534.0, 535.0, 536.0, 537.0, 538.0, 539.0, 540.0, 541.0, 542.0, 543.0, 544.0, 545.0, 546.0, 547.0, 548.0, 549.0, 550.0, 551.0, 552.0, 553.0, 554.0, 555.0, 556.0, 557.0, 558.0, 559.0, 560.0, 561.0, 562.0, 563.0, 564.0, 565.0, 566.0, 567.0, 568.0, 569.0, 570.0, 571.0, 572.0, 573.0, 574.0, 575.0, 576.0, 577.0, 578.0, 579.0, 580.0, 581.0, 582.0, 583.0, 584.0, 585.0, 586.0, 587.0, 588.0, 589.0, 590.0, 591.0, 592.0, 593.0, 594.0, 595.0, 596.0, 597.0, 598.0, 599.0, 600.0, 601.0, 602.0, 603.0, 604.0, 605.0, 606.0, 607.0, 608.0, 609.0, 610.0, 611.0, 612.0, 613.0, 614.0, 615.0, 616.0, 617.0, 618.0, 619.0, 620.0, 621.0, 622.0, 623.0, 624.0, 625.0, 626.0, 627.0, 628.0, 629.0, 630.0, 631.0, 632.0, 633.0, 634.0, 635.0, 636.0, 637.0, 638.0, 639.0, 640.0, 641.0, 642.0, 643.0, 644.0, 645.0, 646.0, 647.0, 648.0, 649.0, 650.0, 651.0, 652.0, 653.0, 654.0, 655.0, 656.0, 657.0, 658.0, 659.0, 660.0, 661.0, 662.0, 663.0, 664.0, 665.0, 666.0, 667.0, 668.0, 669.0, 670.0, 671.0, 672.0, 673.0, 674.0, 675.0, 676.0, 677.0, 678.0, 679.0, 680.0, 681.0, 682.0, 683.0, 684.0, 685.0, 686.0, 687.0, 688.0, 689.0, 690.0, 691.0, 692.0, 693.0, 694.0, 695.0, 696.0, 697.0, 698.0, 699.0, 700.0, 701.0, 702.0, 703.0, 704.0, 705.0, 706.0, 707.0, 708.0, 709.0, 710.0, 711.0, 712.0, 713.0, 714.0, 715.0, 716.0, 717.0, 718.0, 719.0, 720.0, 721.0, 722.0, 723.0, 724.0, 725.0, 726.0, 727.0, 728.0, 729.0, 730.0, 731.0, 732.0, 733.0, 734.0, 735.0, 736.0, 737.0, 738.0, 739.0, 740.0, 741.0, 742.0, 743.0, 744.0, 745.0, 746.0, 747.0, 748.0, 749.0, 750.0, 751.0, 752.0, 753.0, 754.0, 755.0, 756.0, 757.0, 758.0, 759.0, 760.0, 761.0, 762.0, 763.0, 764.0, 765.0, 766.0, 767.0, 768.0, 769.0, 770.0, 771.0, 772.0, 773.0, 774.0, 775.0, 776.0, 777.0, 778.0, 779.0, 780.0, 781.0, 782.0, 783.0, 784.0, 785.0, 786.0, 787.0, 788.0, 789.0, 790.0, 791.0, 792.0, 793.0, 794.0, 795.0, 796.0, 797.0, 798.0, 799.0, 800.0, 801.0, 802.0, 803.0, 804.0, 805.0, 806.0, 807.0, 808.0, 809.0, 810.0, 811.0, 812.0, 813.0, 814.0, 815.0, 816.0, 817.0, 818.0, 819.0, 820.0, 821.0, 822.0, 823.0, 824.0, 825.0, 826.0, 827.0, 828.0, 829.0, 830.0, 831.0, 832.0, 833.0, 834.0, 835.0, 836.0, 837.0, 838.0, 839.0, 840.0, 841.0, 842.0, 843.0, 844.0, 845.0, 846.0, 847.0, 848.0, 849.0, 850.0, 851.0, 852.0, 853.0, 854.0, 855.0, 856.0, 857.0, 858.0, 859.0, 860.0, 861.0, 862.0, 863.0, 864.0, 865.0, 866.0, 867.0, 868.0, 869.0, 870.0, 871.0, 872.0, 873.0, 874.0, 875.0, 876.0, 877.0, 878.0, 879.0, 880.0, 881.0, 882.0, 883.0, 884.0, 885.0, 886.0, 887.0, 888.0, 889.0, 890.0, 891.0, 892.0, 893.0, 894.0, 895.0, 896.0, 897.0, 898.0, 899.0, 900.0, 901.0, 902.0, 903.0, 904.0, 905.0, 906.0, 907.0, 908.0, 909.0, 910.0, 911.0, 912.0, 913.0, 914.0, 915.0, 916.0, 917.0, 918.0, 919.0, 920.0, 921.0, 922.0, 923.0, 924.0, 925.0, 926.0, 927.0, 928.0, 929.0, 930.0, 931.0, 932.0, 933.0, 934.0, 935.0, 936.0, 937.0, 938.0, 939.0, 940.0, 941.0, 942.0, 943.0, 944.0, 945.0, 946.0, 947.0, 948.0, 949.0, 950.0, 951.0, 952.0, 953.0, 954.0, 955.0, 956.0, 957.0, 958.0, 959.0, 960.0, 961.0, 962.0, 963.0, 964.0, 965.0, 966.0, 967.0, 968.0, 969.0, 970.0, 971.0, 972.0, 973.0, 974.0, 975.0, 976.0, 977.0, 978.0, 979.0, 980.0, 981.0, 982.0, 983.0, 984.0, 985.0, 986.0, 987.0, 988.0, 989.0, 990.0, 991.0, 992.0, 993.0, 994.0, 995.0, 996.0, 997.0, 998.0, 999.0, 1000.0, 1000.9999999999999, 1002.0, 1002.9999999999999, 1004.0, 1004.9999999999999, 1006.0, 1006.9999999999999, 1008.0, 1008.9999999999999, 1010.0, 1010.9999999999999, 1012.0, 1012.9999999999999, 1014.0, 1014.9999999999999, 1016.0, 1016.9999999999999, 1018.0, 1018.9999999999999, 1020.0, 1020.9999999999999, 1022.0, 1022.9999999999999, 1024.0, 1025.0, 1026.0, 1027.0, 1028.0, 1029.0, 1030.0, 1031.0, 1032.0, 1033.0, 1034.0, 1035.0, 1036.0, 1037.0, 1038.0, 1039.0, 1040.0, 1041.0, 1042.0, 1043.0, 1044.0, 1045.0, 1046.0, 1047.0, 1048.0, 1049.0, 1050.0, 1051.0, 1052.0, 1053.0, 1054.0, 1055.0, 1056.0, 1057.0, 1058.0, 1059.0, 1060.0, 1061.0, 1062.0, 1063.0, 1064.0, 1065.0, 1066.0, 1067.0, 1068.0, 1069.0, 1070.0, 1071.0, 1072.0, 1073.0, 1074.0, 1075.0, 1076.0, 1077.0, 1078.0, 1079.0, 1080.0, 1081.0, 1082.0, 1083.0, 1084.0, 1085.0, 1086.0, 1087.0, 1088.0, 1089.0, 1090.0, 1091.0, 1092.0, 1093.0, 1094.0, 1095.0, 1096.0, 1097.0, 1098.0, 1099.0, 1100.0, 1101.0, 1102.0, 1103.0, 1104.0, 1105.0, 1106.0, 1107.0, 1108.0, 1109.0, 1110.0, 1111.0, 1112.0, 1113.0, 1114.0, 1115.0, 1116.0, 1117.0, 1118.0, 1119.0, 1120.0, 1121.0, 1122.0, 1123.0, 1124.0, 1125.0, 1126.0, 1127.0, 1128.0, 1129.0, 1130.0, 1131.0, 1132.0, 1133.0, 1134.0, 1135.0, 1136.0, 1137.0, 1138.0, 1139.0, 1140.0, 1141.0, 1142.0, 1143.0, 1144.0, 1145.0, 1146.0, 1147.0, 1148.0, 1149.0, 1150.0, 1151.0, 1152.0, 1153.0, 1154.0, 1155.0, 1156.0, 1157.0, 1158.0, 1159.0, 1160.0, 1161.0, 1162.0, 1163.0, 1164.0, 1165.0, 1166.0, 1167.0, 1168.0, 1169.0, 1170.0, 1171.0, 1172.0, 1173.0, 1174.0, 1175.0, 1176.0, 1177.0, 1178.0, 1179.0, 1180.0, 1181.0, 1182.0, 1183.0, 1184.0, 1185.0, 1186.0, 1187.0, 1188.0, 1189.0, 1190.0, 1191.0, 1192.0, 1193.0, 1194.0, 1195.0, 1196.0, 1197.0, 1198.0, 1199.0, 1200.0, 1201.0, 1202.0, 1203.0, 1204.0, 1205.0, 1206.0, 1207.0, 1208.0, 1209.0, 1210.0, 1211.0, 1212.0, 1213.0, 1214.0, 1215.0, 1216.0, 1217.0, 1218.0, 1219.0, 1220.0, 1221.0, 1222.0, 1223.0, 1224.0, 1225.0, 1226.0, 1227.0, 1228.0, 1229.0, 1230.0, 1231.0, 1232.0, 1233.0, 1234.0, 1235.0, 1236.0, 1237.0, 1238.0, 1239.0, 1240.0, 1241.0, 1242.0, 1243.0, 1244.0, 1245.0, 1246.0, 1247.0, 1248.0, 1249.0, 1250.0, 1251.0, 1252.0, 1253.0, 1254.0, 1255.0, 1256.0, 1257.0, 1258.0, 1259.0, 1260.0, 1261.0, 1262.0, 1263.0, 1264.0, 1265.0, 1266.0, 1267.0, 1268.0, 1269.0, 1270.0, 1271.0, 1272.0, 1273.0, 1274.0, 1275.0, 1276.0, 1277.0, 1278.0, 1279.0, 1280.0, 1281.0, 1282.0, 1283.0, 1284.0, 1285.0, 1286.0, 1287.0, 1288.0, 1289.0, 1290.0, 1291.0, 1292.0, 1293.0, 1294.0, 1295.0, 1296.0, 1297.0, 1298.0, 1299.0, 1300.0, 1301.0, 1302.0, 1303.0, 1304.0, 1305.0, 1306.0, 1307.0, 1308.0, 1309.0, 1310.0, 1311.0, 1312.0, 1313.0, 1314.0, 1315.0, 1316.0, 1317.0, 1318.0, 1319.0, 1320.0, 1321.0, 1322.0, 1323.0, 1324.0, 1325.0, 1326.0, 1327.0, 1328.0, 1329.0, 1330.0, 1331.0, 1332.0, 1333.0, 1334.0, 1335.0, 1336.0, 1337.0, 1338.0, 1339.0, 1340.0, 1341.0, 1342.0, 1343.0, 1344.0, 1345.0, 1346.0, 1347.0, 1348.0, 1349.0, 1350.0, 1351.0, 1352.0, 1353.0, 1354.0, 1355.0, 1356.0, 1357.0, 1358.0, 1359.0, 1360.0, 1361.0, 1362.0, 1363.0, 1364.0, 1365.0, 1366.0, 1367.0, 1368.0, 1369.0, 1370.0, 1371.0, 1372.0, 1373.0, 1374.0, 1375.0, 1376.0, 1377.0, 1378.0, 1379.0, 1380.0, 1381.0, 1382.0, 1383.0, 1384.0, 1385.0, 1386.0, 1387.0, 1388.0, 1389.0, 1390.0, 1391.0, 1392.0, 1393.0, 1394.0, 1395.0, 1396.0, 1397.0, 1398.0, 1399.0, 1400.0, 1401.0, 1402.0, 1403.0, 1404.0, 1405.0, 1406.0, 1407.0, 1408.0, 1409.0, 1410.0, 1411.0, 1412.0, 1413.0, 1414.0, 1415.0, 1416.0, 1417.0, 1418.0, 1419.0, 1420.0, 1421.0, 1422.0, 1423.0, 1424.0, 1425.0, 1426.0, 1427.0, 1428.0, 1429.0, 1430.0, 1431.0, 1432.0, 1433.0, 1434.0, 1435.0, 1436.0, 1437.0, 1438.0, 1439.0, 1440.0, 1441.0, 1442.0, 1443.0, 1444.0, 1445.0, 1446.0, 1447.0, 1448.0, 1449.0, 1450.0, 1451.0, 1452.0, 1453.0, 1454.0, 1455.0, 1456.0, 1457.0, 1458.0, 1459.0, 1460.0, 1461.0, 1462.0, 1463.0, 1464.0, 1465.0, 1466.0, 1467.0, 1468.0, 1469.0, 1470.0, 1471.0, 1472.0, 1473.0, 1474.0, 1475.0, 1476.0, 1477.0, 1478.0, 1479.0, 1480.0, 1481.0, 1482.0, 1483.0, 1484.0, 1485.0, 1486.0, 1487.0, 1488.0, 1489.0, 1490.0, 1491.0, 1492.0, 1493.0, 1494.0, 1495.0, 1496.0, 1497.0, 1498.0, 1499.0, 1500.0, 1501.0, 1502.0, 1503.0, 1504.0, 1505.0, 1506.0, 1507.0, 1508.0, 1509.0, 1510.0, 1511.0, 1512.0, 1513.0, 1514.0, 1515.0, 1516.0, 1517.0, 1518.0, 1519.0, 1520.0, 1521.0, 1522.0, 1523.0, 1524.0, 1525.0, 1526.0, 1527.0, 1528.0, 1529.0, 1530.0, 1531.0, 1532.0, 1533.0, 1534.0, 1535.0, 1536.0, 1537.0, 1538.0, 1539.0, 1540.0, 1541.0, 1542.0, 1543.0, 1544.0, 1545.0, 1546.0, 1547.0, 1548.0, 1549.0, 1550.0, 1551.0, 1552.0, 1553.0, 1554.0, 1555.0, 1556.0, 1557.0, 1558.0, 1559.0, 1560.0, 1561.0, 1562.0, 1563.0, 1564.0, 1565.0, 1566.0, 1567.0, 1568.0, 1569.0, 1570.0, 1571.0, 1572.0, 1573.0, 1574.0, 1575.0, 1576.0, 1577.0, 1578.0, 1579.0, 1580.0, 1581.0, 1582.0, 1583.0, 1584.0, 1585.0, 1586.0, 1587.0, 1588.0, 1589.0, 1590.0, 1591.0, 1592.0, 1593.0, 1594.0, 1595.0, 1596.0, 1597.0, 1598.0, 1599.0, 1600.0, 1601.0, 1602.0, 1603.0, 1604.0, 1605.0, 1606.0, 1607.0, 1608.0, 1609.0, 1610.0, 1611.0, 1612.0, 1613.0, 1614.0, 1615.0, 1616.0, 1617.0, 1618.0, 1619.0, 1620.0, 1621.0, 1622.0, 1623.0, 1624.0, 1625.0, 1626.0, 1627.0, 1628.0, 1629.0, 1630.0, 1631.0, 1632.0, 1633.0, 1634.0, 1635.0, 1636.0, 1637.0, 1638.0, 1639.0, 1640.0, 1641.0, 1642.0, 1643.0, 1644.0, 1645.0, 1646.0, 1647.0, 1648.0, 1649.0, 1650.0, 1651.0, 1652.0, 1653.0, 1654.0, 1655.0, 1656.0, 1657.0, 1658.0, 1659.0, 1660.0, 1661.0, 1662.0, 1663.0, 1664.0, 1665.0, 1666.0, 1667.0, 1668.0, 1669.0, 1670.0, 1671.0, 1672.0, 1673.0, 1674.0, 1675.0, 1676.0, 1677.0, 1678.0, 1679.0, 1680.0, 1681.0, 1682.0, 1683.0, 1684.0, 1685.0, 1686.0, 1687.0, 1688.0, 1689.0, 1690.0, 1691.0, 1692.0, 1693.0, 1694.0, 1695.0, 1696.0, 1697.0, 1698.0, 1699.0, 1700.0, 1701.0, 1702.0, 1703.0, 1704.0, 1705.0, 1706.0, 1707.0, 1708.0, 1709.0, 1710.0, 1711.0, 1712.0, 1713.0, 1714.0, 1715.0, 1716.0, 1717.0, 1718.0, 1719.0, 1720.0, 1721.0, 1722.0, 1723.0, 1724.0, 1725.0, 1726.0, 1727.0, 1728.0, 1729.0, 1730.0, 1731.0, 1732.0, 1733.0, 1734.0, 1735.0, 1736.0, 1737.0, 1738.0, 1739.0, 1740.0, 1741.0, 1742.0, 1743.0, 1744.0, 1745.0, 1746.0, 1747.0, 1748.0, 1749.0, 1750.0), value=-500.0), Dropdown(description=u'thresh', options=(0.001, 0.005, 0.01, 0.05), value=0.001), Output()), _dom_classes=('widget-interact',))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.show_tfce>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def show_tfce(time, thresh):\n",
    "\n",
    "    time_ix = list(times).index(time)\n",
    "    tf = tfce[:, time_ix].squeeze()\n",
    "    mask = np.where(cluster_ps[:, time_ix] <= thresh, True, False)\n",
    "    \n",
    "    # plot average test statistic and mark significant sensors\n",
    "    image, _ = mne.viz.plot_topomap(tf, pos, mask=mask,\n",
    "                                    vmin=0, vmax=30)\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "interact(show_tfce, time=times, thresh=[.001, .005, .01, 0.05])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# TFR Power"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make TFR Power"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Raw TFR Power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-23T16:01:35.744880Z",
     "start_time": "2017-12-22T19:22:10.500982Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base\n",
      "stimulus\n",
      "sub-hc001\n",
      "sub-hc002\n",
      "sub-hc003\n",
      "sub-hc004\n",
      "sub-hc005\n",
      "sub-hc006\n",
      "sub-hc007\n",
      "sub-hc008\n",
      "sub-hc009\n",
      "sub-hc010\n",
      "sub-hc011\n",
      "sub-hc012\n",
      "sub-hc014\n",
      "sub-hc015\n",
      "sub-hc016\n",
      "sub-hc017\n",
      "sub-hc019\n",
      "sub-hc020\n",
      "sub-hc021\n",
      "sub-hc022\n",
      "sub-hc023\n",
      "sub-hc024\n",
      "sub-hc025\n",
      "sub-hc026\n",
      "sub-hc028\n",
      "sub-hc029\n",
      "sub-hc030\n",
      "sub-hc031\n",
      "sub-hc032\n",
      "sub-hc033\n",
      "sub-hc034\n",
      "sub-hc035\n",
      "sub-hc036\n",
      "sub-hc037\n",
      "sub-hc042\n",
      "sub-hc044\n",
      "sub-hc045\n",
      "sub-pp001\n",
      "sub-pp002\n",
      "sub-pp003\n",
      "sub-pp004\n",
      "sub-pp005\n",
      "sub-pp006\n",
      "sub-pp007\n",
      "sub-pp008\n",
      "sub-pp009\n",
      "sub-pp010\n",
      "sub-pp011\n",
      "sub-pp012\n",
      "sub-pp013\n",
      "sub-pp014\n",
      "sub-pp015\n",
      "sub-pp016\n",
      "response\n",
      "sub-hc001\n",
      "sub-hc002\n",
      "sub-hc003\n",
      "sub-hc004\n",
      "sub-hc005\n",
      "sub-hc006\n",
      "sub-hc007\n",
      "sub-hc008\n",
      "sub-hc009\n",
      "sub-hc010\n",
      "sub-hc011\n",
      "sub-hc012\n",
      "sub-hc014\n",
      "sub-hc015\n",
      "sub-hc016\n",
      "sub-hc017\n",
      "sub-hc019\n",
      "sub-hc020\n",
      "sub-hc021\n",
      "sub-hc022\n",
      "sub-hc023\n",
      "sub-hc024\n",
      "sub-hc025\n",
      "sub-hc026\n",
      "sub-hc028\n",
      "sub-hc029\n",
      "sub-hc030\n",
      "sub-hc031\n",
      "sub-hc032\n",
      "sub-hc033\n",
      "sub-hc034\n",
      "sub-hc035\n",
      "sub-hc036\n",
      "sub-hc037\n",
      "sub-hc042\n",
      "sub-hc044\n",
      "sub-hc045\n",
      "sub-pp001\n",
      "sub-pp002\n",
      "sub-pp003\n",
      "sub-pp004\n",
      "sub-pp005\n",
      "sub-pp006\n",
      "sub-pp007\n",
      "sub-pp008\n",
      "sub-pp009\n",
      "sub-pp010\n",
      "sub-pp011\n",
      "sub-pp012\n",
      "sub-pp013\n",
      "sub-pp014\n",
      "sub-pp015\n",
      "sub-pp016\n",
      "laplacian\n",
      "stimulus\n",
      "sub-hc001\n",
      "sub-hc002\n",
      "sub-hc003\n",
      "sub-hc004\n",
      "sub-hc005\n",
      "sub-hc006\n",
      "sub-hc007\n",
      "sub-hc008\n",
      "sub-hc009\n",
      "sub-hc010\n",
      "sub-hc011\n",
      "sub-hc012\n",
      "sub-hc014\n",
      "sub-hc015\n",
      "sub-hc016\n",
      "sub-hc017\n",
      "sub-hc019\n",
      "sub-hc020\n",
      "sub-hc021\n",
      "sub-hc022\n",
      "sub-hc023\n",
      "sub-hc024\n",
      "sub-hc025\n",
      "sub-hc026\n",
      "sub-hc028\n",
      "sub-hc029\n",
      "sub-hc030\n",
      "sub-hc031\n",
      "sub-hc032\n",
      "sub-hc033\n",
      "sub-hc034\n",
      "sub-hc035\n",
      "sub-hc036\n",
      "sub-hc037\n",
      "sub-hc042\n",
      "sub-hc044\n",
      "sub-hc045\n",
      "sub-pp001\n",
      "sub-pp002\n",
      "sub-pp003\n",
      "sub-pp004\n",
      "sub-pp005\n",
      "sub-pp006\n",
      "sub-pp007\n",
      "sub-pp008\n",
      "sub-pp009\n",
      "sub-pp010\n",
      "sub-pp011\n",
      "sub-pp012\n",
      "sub-pp013\n",
      "sub-pp014\n",
      "sub-pp015\n",
      "sub-pp016\n",
      "response\n",
      "sub-hc001\n",
      "sub-hc002\n",
      "sub-hc003\n",
      "sub-hc004\n",
      "sub-hc005\n",
      "sub-hc006\n",
      "sub-hc007\n",
      "sub-hc008\n",
      "sub-hc009\n",
      "sub-hc010\n",
      "sub-hc011\n",
      "sub-hc012\n",
      "sub-hc014\n",
      "sub-hc015\n",
      "sub-hc016\n",
      "sub-hc017\n",
      "sub-hc019\n",
      "sub-hc020\n",
      "sub-hc021\n",
      "sub-hc022\n",
      "sub-hc023\n",
      "sub-hc024\n",
      "sub-hc025\n",
      "sub-hc026\n",
      "sub-hc028\n",
      "sub-hc029\n",
      "sub-hc030\n",
      "sub-hc031\n",
      "sub-hc032\n",
      "sub-hc033\n",
      "sub-hc034\n",
      "sub-hc035\n",
      "sub-hc036\n",
      "sub-hc037\n",
      "sub-hc042\n",
      "sub-hc044\n",
      "sub-hc045\n",
      "sub-pp001\n",
      "sub-pp002\n",
      "sub-pp003\n",
      "sub-pp004\n",
      "sub-pp005\n",
      "sub-pp006\n",
      "sub-pp007\n",
      "sub-pp008\n",
      "sub-pp009\n",
      "sub-pp010\n",
      "sub-pp011\n",
      "sub-pp012\n",
      "sub-pp013\n",
      "sub-pp014\n",
      "sub-pp015\n",
      "sub-pp016\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "for epo_class in ['base', 'laplacian']:\n",
    "    print(epo_class)\n",
    "    \n",
    "    if epo_class == 'base':\n",
    "        epo_root = '../data/derivatives/eeg_preprocessing/%s/epochs/%s_%s_cleaned-epo.fif'\n",
    "    else:\n",
    "        epo_root = '../data/derivatives/eeg_sensor/%s/lap/%s_%s_lap-epo.fif'\n",
    "        \n",
    "    for epo_type, epo_times in zip(epoch_types, epoch_times):\n",
    "        print(epo_type)\n",
    "\n",
    "        for subject in subjects:\n",
    "            if subject == 'group':\n",
    "                continue\n",
    "            print(subject)\n",
    "\n",
    "            # load subject behavior and epochs\n",
    "            sub_behavior = behavior[behavior.participant_id == subject]\n",
    "            f = epo_root % (subject, subject, epo_type)\n",
    "            epochs = read_epochs(f, verbose=False)\n",
    "\n",
    "            # interpolate the bad channels\n",
    "            epochs.interpolate_bads(reset_bads=True)\n",
    "\n",
    "            # drop bad trials from epochs and behavior\n",
    "            sub_behavior, epochs = drop_bad_trials(subject, sub_behavior, \n",
    "                                                   epochs, layout, epo_type)\n",
    "\n",
    "            # add event labels\n",
    "            epochs = add_events(epochs, sub_behavior)\n",
    "\n",
    "            tfrs = []\n",
    "            for event in conditions:\n",
    "                cond_epochs = epochs[event]\n",
    "\n",
    "                # remove evoked response\n",
    "                cond_epochs.subtract_evoked()\n",
    "\n",
    "                # compute tfr\n",
    "                power = tfr_morlet(cond_epochs, freqs=frequencies, \n",
    "                                   n_cycles=n_cycles, decim=5, \n",
    "                                   return_itc=False, average=False, n_jobs=2)\n",
    "\n",
    "                # crop out filter buffer\n",
    "                power.crop(epo_times[0], epo_times[1])\n",
    "                tfrs.append(power)\n",
    "\n",
    "            f = '../data/derivatives/eeg_sensor/%s/tfr/%s_%s_%s_raw-tfr.h5'\n",
    "            write_tfrs(f % (subject, subject, epo_type, epo_class), tfrs, \n",
    "                       overwrite=True)\n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Normalize Power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-23T17:23:32.235848Z",
     "start_time": "2017-12-23T16:01:38.966608Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stimulus\n",
      "sub-hc001\n",
      "sub-hc002\n",
      "sub-hc003\n",
      "sub-hc004\n",
      "sub-hc005\n",
      "sub-hc006\n",
      "sub-hc007\n",
      "sub-hc008\n",
      "sub-hc009\n",
      "sub-hc010\n",
      "sub-hc011\n",
      "sub-hc012\n",
      "sub-hc014\n",
      "sub-hc015\n",
      "sub-hc016\n",
      "sub-hc017\n",
      "sub-hc019\n",
      "sub-hc020\n",
      "sub-hc021\n",
      "sub-hc022\n",
      "sub-hc023\n",
      "sub-hc024\n",
      "sub-hc025\n",
      "sub-hc026\n",
      "sub-hc028\n",
      "sub-hc029\n",
      "sub-hc030\n",
      "sub-hc031\n",
      "sub-hc032\n",
      "sub-hc033\n",
      "sub-hc034\n",
      "sub-hc035\n",
      "sub-hc036\n",
      "sub-hc037\n",
      "sub-hc042\n",
      "sub-hc044\n",
      "sub-hc045\n",
      "sub-pp001\n",
      "sub-pp002\n",
      "sub-pp003\n",
      "sub-pp004\n",
      "sub-pp005\n",
      "sub-pp006\n",
      "sub-pp007\n",
      "sub-pp008\n",
      "sub-pp009\n",
      "sub-pp010\n",
      "sub-pp011\n",
      "sub-pp012\n",
      "sub-pp013\n",
      "sub-pp014\n",
      "sub-pp015\n",
      "sub-pp016\n",
      "response\n",
      "sub-hc001\n",
      "sub-hc002\n",
      "sub-hc003\n",
      "sub-hc004\n",
      "sub-hc005\n",
      "sub-hc006\n",
      "sub-hc007\n",
      "sub-hc008\n",
      "sub-hc009\n",
      "sub-hc010\n",
      "sub-hc011\n",
      "sub-hc012\n",
      "sub-hc014\n",
      "sub-hc015\n",
      "sub-hc016\n",
      "sub-hc017\n",
      "sub-hc019\n",
      "sub-hc020\n",
      "sub-hc021\n",
      "sub-hc022\n",
      "sub-hc023\n",
      "sub-hc024\n",
      "sub-hc025\n",
      "sub-hc026\n",
      "sub-hc028\n",
      "sub-hc029\n",
      "sub-hc030\n",
      "sub-hc031\n",
      "sub-hc032\n",
      "sub-hc033\n",
      "sub-hc034\n",
      "sub-hc035\n",
      "sub-hc036\n",
      "sub-hc037\n",
      "sub-hc042\n",
      "sub-hc044\n",
      "sub-hc045\n",
      "sub-pp001\n",
      "sub-pp002\n",
      "sub-pp003\n",
      "sub-pp004\n",
      "sub-pp005\n",
      "sub-pp006\n",
      "sub-pp007\n",
      "sub-pp008\n",
      "sub-pp009\n",
      "sub-pp010\n",
      "sub-pp011\n",
      "sub-pp012\n",
      "sub-pp013\n",
      "sub-pp014\n",
      "sub-pp015\n",
      "sub-pp016\n",
      "stimulus\n",
      "sub-hc001\n",
      "sub-hc002\n",
      "sub-hc003\n",
      "sub-hc004\n",
      "sub-hc005\n",
      "sub-hc006\n",
      "sub-hc007\n",
      "sub-hc008\n",
      "sub-hc009\n",
      "sub-hc010\n",
      "sub-hc011\n",
      "sub-hc012\n",
      "sub-hc014\n",
      "sub-hc015\n",
      "sub-hc016\n",
      "sub-hc017\n",
      "sub-hc019\n",
      "sub-hc020\n",
      "sub-hc021\n",
      "sub-hc022\n",
      "sub-hc023\n",
      "sub-hc024\n",
      "sub-hc025\n",
      "sub-hc026\n",
      "sub-hc028\n",
      "sub-hc029\n",
      "sub-hc030\n",
      "sub-hc031\n",
      "sub-hc032\n",
      "sub-hc033\n",
      "sub-hc034\n",
      "sub-hc035\n",
      "sub-hc036\n",
      "sub-hc037\n",
      "sub-hc042\n",
      "sub-hc044\n",
      "sub-hc045\n",
      "sub-pp001\n",
      "sub-pp002\n",
      "sub-pp003\n",
      "sub-pp004\n",
      "sub-pp005\n",
      "sub-pp006\n",
      "sub-pp007\n",
      "sub-pp008\n",
      "sub-pp009\n",
      "sub-pp010\n",
      "sub-pp011\n",
      "sub-pp012\n",
      "sub-pp013\n",
      "sub-pp014\n",
      "sub-pp015\n",
      "sub-pp016\n",
      "response\n",
      "sub-hc001\n",
      "sub-hc002\n",
      "sub-hc003\n",
      "sub-hc004\n",
      "sub-hc005\n",
      "sub-hc006\n",
      "sub-hc007\n",
      "sub-hc008\n",
      "sub-hc009\n",
      "sub-hc010\n",
      "sub-hc011\n",
      "sub-hc012\n",
      "sub-hc014\n",
      "sub-hc015\n",
      "sub-hc016\n",
      "sub-hc017\n",
      "sub-hc019\n",
      "sub-hc020\n",
      "sub-hc021\n",
      "sub-hc022\n",
      "sub-hc023\n",
      "sub-hc024\n",
      "sub-hc025\n",
      "sub-hc026\n",
      "sub-hc028\n",
      "sub-hc029\n",
      "sub-hc030\n",
      "sub-hc031\n",
      "sub-hc032\n",
      "sub-hc033\n",
      "sub-hc034\n",
      "sub-hc035\n",
      "sub-hc036\n",
      "sub-hc037\n",
      "sub-hc042\n",
      "sub-hc044\n",
      "sub-hc045\n",
      "sub-pp001\n",
      "sub-pp002\n",
      "sub-pp003\n",
      "sub-pp004\n",
      "sub-pp005\n",
      "sub-pp006\n",
      "sub-pp007\n",
      "sub-pp008\n",
      "sub-pp009\n",
      "sub-pp010\n",
      "sub-pp011\n",
      "sub-pp012\n",
      "sub-pp013\n",
      "sub-pp014\n",
      "sub-pp015\n",
      "sub-pp016\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# cache stimulus baseline periods \n",
    "baselines = {}\n",
    "\n",
    "for epo_class in ['base', 'laplacian']:\n",
    "    for epo_type, epo_times in zip(epoch_types, epoch_times):\n",
    "        print(epo_type)\n",
    "\n",
    "        group = {'incongruent': [], 'congruent': []}\n",
    "        for subject in subjects:\n",
    "            if subject == 'group':\n",
    "                continue\n",
    "            print(subject)\n",
    "\n",
    "            f = '../data/derivatives/eeg_sensor/%s/tfr/%s_%s_%s_raw-tfr.h5'\n",
    "            tfrs = read_tfrs(f % (subject, subject, epo_type, epo_class))\n",
    "\n",
    "            if epo_type == 'stimulus':\n",
    "                baselines[subject] = {'incongruent': (-.5, -.1), \n",
    "                                      'congruent': (-.5, -.1)}\n",
    "\n",
    "            norm_tfrs = []\n",
    "            for i, c in enumerate(conditions):\n",
    "\n",
    "                tfr, baseline = baseline_normalize(tfrs[i], \n",
    "                                                   baselines[subject][c],\n",
    "                                                   func=np.median, \n",
    "                                                   method='classic')\n",
    "                norm_tfrs.append(tfr)\n",
    "                group[c].append(tfr)\n",
    "                baselines[subject][c] = baseline\n",
    "\n",
    "\n",
    "            f = '../data/derivatives/eeg_sensor/%s/tfr/%s_%s_%s_norm-tfr.h5'\n",
    "            write_tfrs(f % (subject, subject, epo_type, epo_class), norm_tfrs, \n",
    "                       overwrite=True)\n",
    "\n",
    "        group_tfrs = [grand_average(group[c]) for c in conditions]\n",
    "        f = '../data/derivatives/eeg_sensor/group/tfr/group_%s_%s_norm-tfr.h5'\n",
    "        write_tfrs(f % (epo_type, epo_class), group_tfrs, overwrite=True)\n",
    "\n",
    "del baselines\n",
    "del group_tfrs\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize TFR Power"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFR Power Heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-03T01:08:43.288569Z",
     "start_time": "2018-01-03T01:08:36.386063Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f460417d5c04eea97b93b73153aa159",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>interactive</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "interactive(children=(Dropdown(description=u'subject', options=('group', 'sub-hc001', 'sub-hc002', 'sub-hc003', 'sub-hc004', 'sub-hc005', 'sub-hc006', 'sub-hc007', 'sub-hc008', 'sub-hc009', 'sub-hc010', 'sub-hc011', 'sub-hc012', 'sub-hc014', 'sub-hc015', 'sub-hc016', 'sub-hc017', 'sub-hc019', 'sub-hc020', 'sub-hc021', 'sub-hc022', 'sub-hc023', 'sub-hc024', 'sub-hc025', 'sub-hc026', 'sub-hc028', 'sub-hc029', 'sub-hc030', 'sub-hc031', 'sub-hc032', 'sub-hc033', 'sub-hc034', 'sub-hc035', 'sub-hc036', 'sub-hc037', 'sub-hc042', 'sub-hc044', 'sub-hc045', 'sub-pp001', 'sub-pp002', 'sub-pp003', 'sub-pp004', 'sub-pp005', 'sub-pp006', 'sub-pp007', 'sub-pp008', 'sub-pp009', 'sub-pp010', 'sub-pp011', 'sub-pp012', 'sub-pp013', 'sub-pp014', 'sub-pp015', 'sub-pp016'), value='group'), Dropdown(description=u'ch', options=('Fp1', 'Fpz', 'Fp2', 'AF7', 'AF3', 'AFz', 'AF4', 'AF8', 'F7', 'F5', 'F3', 'F1', 'Fz', 'F2', 'F4', 'F6', 'F8', 'FT9', 'FT7', 'FC5', 'FC3', 'FC1', 'FCz', 'FC2', 'FC4', 'FC6', 'FT8', 'FT10', 'T9', 'T7', 'C5', 'C3', 'C1', 'Cz', 'C2', 'C4', 'C6', 'T8', 'T10', 'TP9', 'TP7', 'CP5', 'CP3', 'CP1', 'CPz', 'CP2', 'CP4', 'CP6', 'TP8', 'TP10', 'P9', 'P7', 'P5', 'P3', 'P1', 'Pz', 'P2', 'P4', 'P6', 'P8', 'P10', 'PO7', 'PO3', 'P0z', 'PO4', 'PO8', 'O1', 'Oz', 'O2', 'Iz'), value='Fp1'), FloatSlider(value=2.0, description=u'lim', max=4.0, min=0.5, step=0.5), Dropdown(description=u'epo_class', options=('base', 'laplacian'), value='base'), Output()), _dom_classes=('widget-interact',))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_tfr_heatmap(subject, ch, lim, epo_class, behavior):\n",
    "    \n",
    "    exclusions = ['fast_rt', 'no_response', 'error', 'post_error']\n",
    "    sub_behavior = behavior.loc[np.where(np.sum(behavior[exclusions], \n",
    "                                            axis=1) == 0)[0], :]\n",
    "    if subject != 'group':\n",
    "        sub_behavior = sub_behavior.loc[sub_behavior.participant_id == subject, :]\n",
    "    sns.set(style='white', font_scale=2)\n",
    "    plt.close('all')\n",
    "\n",
    "    fig, axs = plt.subplots(2, 3, figsize=(24, 16))\n",
    "    \n",
    "    \n",
    "    for i, epo_type in enumerate(epoch_types):\n",
    "        \n",
    "        f = '../data/derivatives/eeg_sensor/%s/tfr/%s_%s_%s_norm-tfr.h5'\n",
    "        tfrs = read_tfrs(f % (subject, subject, epo_type, epo_class))\n",
    "\n",
    "        powers = []\n",
    "        for j, c in enumerate(conditions):\n",
    "            \n",
    "            power = tfrs[j]\n",
    "            power.pick_channels([ch])\n",
    "            ax = axs[i, j]\n",
    "            rts = [sub_behavior[sub_behavior.trial_type == c].response_time]\n",
    "            rt_colors = [colors[j]] \n",
    "            if epo_type == 'stimulus':\n",
    "                ax = power_heatmap(power, ax, lim, rts, rt_colors)\n",
    "            else:\n",
    "                ax = power_heatmap(power, ax, lim)\n",
    "            ax.set_title('%s %s-locked' % (c, epo_type))\n",
    "                \n",
    "\n",
    "        ax = axs[i, 2]\n",
    "        power = tfrs[0] - tfrs[1] \n",
    "        power.pick_channels([ch])\n",
    "        rts = [sub_behavior[sub_behavior.trial_type == c].response_time for c in conditions]\n",
    "        rt_colors = [colors[0], colors[1]]\n",
    "        if epo_type == 'stimulus':\n",
    "            ax = power_heatmap(power, ax, lim, rts, rt_colors)\n",
    "        else:\n",
    "            ax = power_heatmap(power, ax, lim)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=.92)\n",
    "    plt.suptitle('%s %s TFR Heatmaps (Color Limit: +- %s)' % (subject, ch, lim), \n",
    "                 fontsize=24)\n",
    "    plt.show()\n",
    "\n",
    "interact(plot_tfr_heatmap, subject=subjects, ch=CH_NAMES, \n",
    "         lim=(.5, 4, .5), epo_class=['base', 'laplacian'], \n",
    "         behavior=fixed(behavior));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFR Power Topomaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-27T16:29:00.153144Z",
     "start_time": "2017-12-27T16:28:57.311419Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e5c65b945044cbda68858d5892126b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>interactive</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "interactive(children=(Dropdown(description=u'subject', options=('group', 'group', 'sub-hc001', 'sub-hc002', 'sub-hc003', 'sub-hc004', 'sub-hc005', 'sub-hc006', 'sub-hc007', 'sub-hc008', 'sub-hc009', 'sub-hc010', 'sub-hc011', 'sub-hc012', 'sub-hc014', 'sub-hc015', 'sub-hc016', 'sub-hc017', 'sub-hc019', 'sub-hc020', 'sub-hc021', 'sub-hc022', 'sub-hc023', 'sub-hc024', 'sub-hc025', 'sub-hc026', 'sub-hc028', 'sub-hc029', 'sub-hc030', 'sub-hc031', 'sub-hc032', 'sub-hc033', 'sub-hc034', 'sub-hc035', 'sub-hc036', 'sub-hc037', 'sub-hc042', 'sub-hc044', 'sub-hc045', 'sub-pp001', 'sub-pp002', 'sub-pp003', 'sub-pp004', 'sub-pp005', 'sub-pp006', 'sub-pp007', 'sub-pp008', 'sub-pp009', 'sub-pp010', 'sub-pp011', 'sub-pp012', 'sub-pp013', 'sub-pp014', 'sub-pp015', 'sub-pp016'), value='group'), Dropdown(description=u'epo_type', options=('response', 'stimulus'), value='response'), Dropdown(description=u'epo_class', options=('base', 'laplacian'), value='base'), Dropdown(description=u'time', options=(-1.0, -0.98999999999999999, -0.97999999999999998, -0.96999999999999997, -0.95999999999999996, -0.94999999999999996, -0.93999999999999995, -0.92999999999999994, -0.91999999999999993, -0.90999999999999992, -0.89999999999999991, -0.8899999999999999, -0.87999999999999989, -0.86999999999999988, -0.85999999999999988, -0.84999999999999987, -0.83999999999999986, -0.82999999999999985, -0.81999999999999984, -0.80999999999999983, -0.79999999999999982, -0.78999999999999981, -0.7799999999999998, -0.7699999999999998, -0.75999999999999979, -0.74999999999999978, -0.73999999999999977, -0.72999999999999976, -0.71999999999999975, -0.70999999999999974, -0.69999999999999973, -0.68999999999999972, -0.67999999999999972, -0.66999999999999971, -0.6599999999999997, -0.64999999999999969, -0.63999999999999968, -0.62999999999999967, -0.61999999999999966, -0.60999999999999965, -0.59999999999999964, -0.58999999999999964, -0.57999999999999963, -0.56999999999999962, -0.55999999999999961, -0.5499999999999996, -0.53999999999999959, -0.52999999999999958, -0.51999999999999957, -0.50999999999999956, -0.49999999999999956, -0.48999999999999955, -0.47999999999999954, -0.46999999999999953, -0.45999999999999952, -0.44999999999999951, -0.4399999999999995, -0.42999999999999949, -0.41999999999999948, -0.40999999999999948, -0.39999999999999947, -0.38999999999999946, -0.37999999999999945, -0.36999999999999944, -0.35999999999999943, -0.34999999999999942, -0.33999999999999941, -0.3299999999999994, -0.3199999999999994, -0.30999999999999939, -0.29999999999999938, -0.28999999999999937, -0.27999999999999936, -0.26999999999999935, -0.25999999999999934, -0.24999999999999933, -0.23999999999999932, -0.22999999999999932, -0.21999999999999931, -0.2099999999999993, -0.19999999999999929, -0.18999999999999928, -0.17999999999999927, -0.16999999999999926, -0.15999999999999925, -0.14999999999999925, -0.13999999999999924, -0.12999999999999923, -0.11999999999999922, -0.10999999999999921, -0.099999999999999201, -0.089999999999999192, -0.079999999999999183, -0.069999999999999174, -0.059999999999999165, -0.049999999999999156, -0.039999999999999147, -0.029999999999999138, -0.01999999999999913, -0.0099999999999991207, 8.8817841970012523e-16, 0.010000000000000897, 0.020000000000000906, 0.030000000000000915, 0.040000000000000924, 0.050000000000000933, 0.060000000000000941, 0.07000000000000095, 0.080000000000000959, 0.090000000000000968, 0.10000000000000098, 0.11000000000000099, 0.12000000000000099, 0.130000000000001, 0.14000000000000101, 0.15000000000000102, 0.16000000000000103, 0.17000000000000104, 0.18000000000000105, 0.19000000000000106, 0.20000000000000107, 0.21000000000000107, 0.22000000000000108, 0.23000000000000109, 0.2400000000000011, 0.25000000000000111, 0.26000000000000112, 0.27000000000000113, 0.28000000000000114, 0.29000000000000115, 0.30000000000000115, 0.31000000000000116, 0.32000000000000117, 0.33000000000000118, 0.34000000000000119, 0.3500000000000012, 0.36000000000000121, 0.37000000000000122, 0.38000000000000123, 0.39000000000000123, 0.40000000000000124, 0.41000000000000125, 0.42000000000000126, 0.43000000000000127, 0.44000000000000128, 0.45000000000000129, 0.4600000000000013, 0.47000000000000131, 0.48000000000000131, 0.49000000000000132, 0.50000000000000133, 0.51000000000000134, 0.52000000000000135, 0.53000000000000136, 0.54000000000000137, 0.55000000000000138, 0.56000000000000139, 0.57000000000000139, 0.5800000000000014, 0.59000000000000141, 0.60000000000000142, 0.61000000000000143, 0.62000000000000144, 0.63000000000000145, 0.64000000000000146, 0.65000000000000147, 0.66000000000000147, 0.67000000000000148, 0.68000000000000149, 0.6900000000000015, 0.70000000000000151, 0.71000000000000152, 0.72000000000000153, 0.73000000000000154, 0.74000000000000155, 0.75000000000000155, 0.76000000000000156, 0.77000000000000157, 0.78000000000000158, 0.79000000000000159, 0.8000000000000016, 0.81000000000000161, 0.82000000000000162, 0.83000000000000163, 0.84000000000000163, 0.85000000000000164, 0.86000000000000165, 0.87000000000000166, 0.88000000000000167, 0.89000000000000168, 0.90000000000000169, 0.9100000000000017, 0.92000000000000171, 0.93000000000000171, 0.94000000000000172, 0.95000000000000173, 0.96000000000000174, 0.97000000000000175, 0.98000000000000176, 0.99000000000000177, 1.0000000000000018, 1.0100000000000016, 1.0200000000000018, 1.030000000000002, 1.0400000000000018, 1.0500000000000016, 1.0600000000000018, 1.0700000000000021, 1.0800000000000018, 1.0900000000000016, 1.1000000000000019, 1.1100000000000021, 1.1200000000000019, 1.1300000000000017, 1.1400000000000019, 1.1500000000000021, 1.1600000000000019, 1.1700000000000017, 1.1800000000000019, 1.1900000000000022, 1.200000000000002, 1.2100000000000017, 1.220000000000002, 1.2300000000000022, 1.240000000000002, 1.2500000000000018, 1.260000000000002, 1.2700000000000022, 1.280000000000002, 1.2900000000000018, 1.300000000000002, 1.3100000000000023, 1.3200000000000021, 1.3300000000000018, 1.3400000000000021, 1.3500000000000023, 1.3600000000000021, 1.3700000000000019, 1.3800000000000021, 1.3900000000000023, 1.4000000000000021, 1.4100000000000019, 1.4200000000000021, 1.4300000000000024, 1.4400000000000022, 1.450000000000002, 1.4600000000000022, 1.4700000000000024, 1.4800000000000022, 1.490000000000002, 1.5000000000000022, 1.5100000000000025, 1.5200000000000022, 1.530000000000002, 1.5400000000000023, 1.5500000000000025, 1.5600000000000023, 1.5700000000000021, 1.5800000000000023, 1.5900000000000025, 1.6000000000000023, 1.6100000000000021, 1.6200000000000023, 1.6300000000000026, 1.6400000000000023, 1.6500000000000021, 1.6600000000000024, 1.6700000000000026, 1.6800000000000024, 1.6900000000000022, 1.7000000000000024, 1.7100000000000026, 1.7200000000000024, 1.7300000000000022, 1.7400000000000024), value=-1.0), Dropdown(description=u'fmin', options=(2.0, 2.2488744551392053, 2.5287181574888282, 2.8433848343116517, 3.1972077600068474, 3.5950594296261187, 4.0424186579967492, 4.5454460284734983, 5.1110687303240008, 5.7470759530432085, 6.4622261512718371, 7.2663676574638858, 8.1705743032601212, 9.1872979172092464, 10.330539798882747, 11.616043530753158, 13.061511783047896, 14.686850097198077, 16.514441005023748, 18.569452258550662, 20.880183415090805, 23.478455550409507, 26.40004946671861, 29.685198430057429, 33.379142222547301, 37.532750139372546, 42.203221509778693, 47.454873388961353, 53.360026268150207, 60.0), value=2.0), Dropdown(description=u'fmax', options=(2.0, 2.2488744551392053, 2.5287181574888282, 2.8433848343116517, 3.1972077600068474, 3.5950594296261187, 4.0424186579967492, 4.5454460284734983, 5.1110687303240008, 5.7470759530432085, 6.4622261512718371, 7.2663676574638858, 8.1705743032601212, 9.1872979172092464, 10.330539798882747, 11.616043530753158, 13.061511783047896, 14.686850097198077, 16.514441005023748, 18.569452258550662, 20.880183415090805, 23.478455550409507, 26.40004946671861, 29.685198430057429, 33.379142222547301, 37.532750139372546, 42.203221509778693, 47.454873388961353, 53.360026268150207, 60.0), value=2.0), Dropdown(description=u'col_limit', options=(0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0, 4.5), value=0.5), Output()), _dom_classes=('widget-interact',))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_topomap(subject, epo_type, epo_class, time, fmin, fmax, col_limit):\n",
    "    plt.close('all')\n",
    "    f = '../data/derivatives/eeg_sensor/%s/tfr/%s_%s_%s_norm-tfr.h5'\n",
    "    tfrs = read_tfrs(f % (subject, subject, epo_type, epo_class))\n",
    "\n",
    "    f, axs = plt.subplots(1, 3, figsize=(24, 6)) \n",
    "    \n",
    "    for i, tfr in enumerate(tfrs):\n",
    "        tfr.plot_topomap(tmin=time, tmax=time + .005,\n",
    "                         fmin=fmin, fmax=fmax, axes=axs[i], colorbar=True, \n",
    "                         show=False, vmin=-col_limit, vmax=col_limit)\n",
    "        axs[i].set_xlabel(conditions[i])\n",
    "        \n",
    "    diff = tfrs[0] - tfrs[1]\n",
    "    diff.plot_topomap(tmin=time, tmax=time + .005,\n",
    "                      fmin=fmin, fmax=fmax, axes=axs[2], colorbar=True, \n",
    "                      show=False, vmin=-col_limit, vmax=col_limit)\n",
    "    axs[2].set_xlabel('i-c')\n",
    "    plt.suptitle('%.2f s' % time)\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "interact(plot_topomap, subject=['group'] + subjects, \n",
    "         epo_type=['response', 'stimulus'],\n",
    "         epo_class=['base', 'laplacian'],\n",
    "         time=np.arange(-1, 1.75, .01), \n",
    "         fmin=frequencies, fmax=frequencies,\n",
    "         col_limit=np.arange(.5, 5, .5));\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "toc": {
   "nav_menu": {
    "height": "30px",
    "width": "252px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {
    "height": "795px",
    "left": "0px",
    "right": "1496.28px",
    "top": "106px",
    "width": "280px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
